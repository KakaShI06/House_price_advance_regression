{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= pd.read_csv('E:\\study material\\programming\\ML\\Datasets\\House_price Regression\\\\train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 81 columns):\n",
      "Id               1460 non-null int64\n",
      "MSSubClass       1460 non-null int64\n",
      "MSZoning         1460 non-null object\n",
      "LotFrontage      1201 non-null float64\n",
      "LotArea          1460 non-null int64\n",
      "Street           1460 non-null object\n",
      "Alley            91 non-null object\n",
      "LotShape         1460 non-null object\n",
      "LandContour      1460 non-null object\n",
      "Utilities        1460 non-null object\n",
      "LotConfig        1460 non-null object\n",
      "LandSlope        1460 non-null object\n",
      "Neighborhood     1460 non-null object\n",
      "Condition1       1460 non-null object\n",
      "Condition2       1460 non-null object\n",
      "BldgType         1460 non-null object\n",
      "HouseStyle       1460 non-null object\n",
      "OverallQual      1460 non-null int64\n",
      "OverallCond      1460 non-null int64\n",
      "YearBuilt        1460 non-null int64\n",
      "YearRemodAdd     1460 non-null int64\n",
      "RoofStyle        1460 non-null object\n",
      "RoofMatl         1460 non-null object\n",
      "Exterior1st      1460 non-null object\n",
      "Exterior2nd      1460 non-null object\n",
      "MasVnrType       1452 non-null object\n",
      "MasVnrArea       1452 non-null float64\n",
      "ExterQual        1460 non-null object\n",
      "ExterCond        1460 non-null object\n",
      "Foundation       1460 non-null object\n",
      "BsmtQual         1423 non-null object\n",
      "BsmtCond         1423 non-null object\n",
      "BsmtExposure     1422 non-null object\n",
      "BsmtFinType1     1423 non-null object\n",
      "BsmtFinSF1       1460 non-null int64\n",
      "BsmtFinType2     1422 non-null object\n",
      "BsmtFinSF2       1460 non-null int64\n",
      "BsmtUnfSF        1460 non-null int64\n",
      "TotalBsmtSF      1460 non-null int64\n",
      "Heating          1460 non-null object\n",
      "HeatingQC        1460 non-null object\n",
      "CentralAir       1460 non-null object\n",
      "Electrical       1459 non-null object\n",
      "1stFlrSF         1460 non-null int64\n",
      "2ndFlrSF         1460 non-null int64\n",
      "LowQualFinSF     1460 non-null int64\n",
      "GrLivArea        1460 non-null int64\n",
      "BsmtFullBath     1460 non-null int64\n",
      "BsmtHalfBath     1460 non-null int64\n",
      "FullBath         1460 non-null int64\n",
      "HalfBath         1460 non-null int64\n",
      "BedroomAbvGr     1460 non-null int64\n",
      "KitchenAbvGr     1460 non-null int64\n",
      "KitchenQual      1460 non-null object\n",
      "TotRmsAbvGrd     1460 non-null int64\n",
      "Functional       1460 non-null object\n",
      "Fireplaces       1460 non-null int64\n",
      "FireplaceQu      770 non-null object\n",
      "GarageType       1379 non-null object\n",
      "GarageYrBlt      1379 non-null float64\n",
      "GarageFinish     1379 non-null object\n",
      "GarageCars       1460 non-null int64\n",
      "GarageArea       1460 non-null int64\n",
      "GarageQual       1379 non-null object\n",
      "GarageCond       1379 non-null object\n",
      "PavedDrive       1460 non-null object\n",
      "WoodDeckSF       1460 non-null int64\n",
      "OpenPorchSF      1460 non-null int64\n",
      "EnclosedPorch    1460 non-null int64\n",
      "3SsnPorch        1460 non-null int64\n",
      "ScreenPorch      1460 non-null int64\n",
      "PoolArea         1460 non-null int64\n",
      "PoolQC           7 non-null object\n",
      "Fence            281 non-null object\n",
      "MiscFeature      54 non-null object\n",
      "MiscVal          1460 non-null int64\n",
      "MoSold           1460 non-null int64\n",
      "YrSold           1460 non-null int64\n",
      "SaleType         1460 non-null object\n",
      "SaleCondition    1460 non-null object\n",
      "SalePrice        1460 non-null int64\n",
      "dtypes: float64(3), int64(35), object(43)\n",
      "memory usage: 924.0+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= dataset.drop(['MiscFeature','Fence', 'PoolQC','Alley','Id','FireplaceQu'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= dataset['SalePrice']\n",
    "dataset= dataset.drop(['SalePrice'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>...</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>35</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>...</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea Street LotShape LandContour  \\\n",
       "0          60       RL         65.0     8450   Pave      Reg         Lvl   \n",
       "1          20       RL         80.0     9600   Pave      Reg         Lvl   \n",
       "2          60       RL         68.0    11250   Pave      IR1         Lvl   \n",
       "3          70       RL         60.0     9550   Pave      IR1         Lvl   \n",
       "4          60       RL         84.0    14260   Pave      IR1         Lvl   \n",
       "\n",
       "  Utilities LotConfig LandSlope  ... OpenPorchSF EnclosedPorch 3SsnPorch  \\\n",
       "0    AllPub    Inside       Gtl  ...          61             0         0   \n",
       "1    AllPub       FR2       Gtl  ...           0             0         0   \n",
       "2    AllPub    Inside       Gtl  ...          42             0         0   \n",
       "3    AllPub    Corner       Gtl  ...          35           272         0   \n",
       "4    AllPub       FR2       Gtl  ...          84             0         0   \n",
       "\n",
       "  ScreenPorch PoolArea  MiscVal  MoSold  YrSold  SaleType SaleCondition  \n",
       "0           0        0        0       2    2008        WD        Normal  \n",
       "1           0        0        0       5    2007        WD        Normal  \n",
       "2           0        0        0       9    2008        WD        Normal  \n",
       "3           0        0        0       2    2006        WD       Abnorml  \n",
       "4           0        0        0      12    2008        WD        Normal  \n",
       "\n",
       "[5 rows x 74 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##In Lotfrontage replacing missing values by mean of the column\n",
    "\n",
    "dataset['LotFrontage']= dataset['LotFrontage'].fillna(dataset['LotFrontage'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None       864\n",
       "BrkFace    445\n",
       "Stone      128\n",
       "BrkCmn      15\n",
       "Name: MasVnrType, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['MasVnrType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Since in 'MasVnrType' only 8 values are missing we can replace it by mode\n",
    "\n",
    "dataset['MasVnrType']= dataset['MasVnrType'].fillna(dataset['MasVnrType'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Since in 'MasVnrValue' only 8 values are missing and all were same values as MasVnrtype which were replace by none hence 0 val\n",
    "\n",
    "dataset['MasVnrArea']= dataset['MasVnrArea'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['BsmtCond']=dataset['BsmtCond'].fillna(dataset['BsmtCond'].mode()[0])\n",
    "dataset['BsmtQual']=dataset['BsmtQual'].fillna(dataset['BsmtQual'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['GarageType']=dataset['GarageType'].fillna(dataset['GarageType'].mode()[0])\n",
    "dataset['GarageFinish']=dataset['GarageFinish'].fillna(dataset['GarageFinish'].mode()[0])\n",
    "dataset['GarageQual']=dataset['GarageQual'].fillna(dataset['GarageQual'].mode()[0])\n",
    "dataset['GarageCond']=dataset['GarageCond'].fillna(dataset['GarageCond'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['BsmtExposure']=dataset['BsmtExposure'].fillna(dataset['BsmtExposure'].mode()[0])\n",
    "dataset['BsmtCond']=dataset['BsmtCond'].fillna(dataset['BsmtCond'].mode()[0])\n",
    "dataset['GarageYrBlt']=dataset['GarageYrBlt'].fillna(dataset['GarageYrBlt'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['BsmtFinType1']=dataset['BsmtFinType1'].fillna(dataset['BsmtFinType1'].mode()[0])\n",
    "dataset['BsmtFinType2']=dataset['BsmtFinType2'].fillna(dataset['BsmtFinType2'].mode()[0])\n",
    "dataset['Electrical']=dataset['Electrical'].fillna(dataset['Electrical'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 74 columns):\n",
      "MSSubClass       1460 non-null int64\n",
      "MSZoning         1460 non-null object\n",
      "LotFrontage      1460 non-null float64\n",
      "LotArea          1460 non-null int64\n",
      "Street           1460 non-null object\n",
      "LotShape         1460 non-null object\n",
      "LandContour      1460 non-null object\n",
      "Utilities        1460 non-null object\n",
      "LotConfig        1460 non-null object\n",
      "LandSlope        1460 non-null object\n",
      "Neighborhood     1460 non-null object\n",
      "Condition1       1460 non-null object\n",
      "Condition2       1460 non-null object\n",
      "BldgType         1460 non-null object\n",
      "HouseStyle       1460 non-null object\n",
      "OverallQual      1460 non-null int64\n",
      "OverallCond      1460 non-null int64\n",
      "YearBuilt        1460 non-null int64\n",
      "YearRemodAdd     1460 non-null int64\n",
      "RoofStyle        1460 non-null object\n",
      "RoofMatl         1460 non-null object\n",
      "Exterior1st      1460 non-null object\n",
      "Exterior2nd      1460 non-null object\n",
      "MasVnrType       1460 non-null object\n",
      "MasVnrArea       1460 non-null float64\n",
      "ExterQual        1460 non-null object\n",
      "ExterCond        1460 non-null object\n",
      "Foundation       1460 non-null object\n",
      "BsmtQual         1460 non-null object\n",
      "BsmtCond         1460 non-null object\n",
      "BsmtExposure     1460 non-null object\n",
      "BsmtFinType1     1460 non-null object\n",
      "BsmtFinSF1       1460 non-null int64\n",
      "BsmtFinType2     1460 non-null object\n",
      "BsmtFinSF2       1460 non-null int64\n",
      "BsmtUnfSF        1460 non-null int64\n",
      "TotalBsmtSF      1460 non-null int64\n",
      "Heating          1460 non-null object\n",
      "HeatingQC        1460 non-null object\n",
      "CentralAir       1460 non-null object\n",
      "Electrical       1460 non-null object\n",
      "1stFlrSF         1460 non-null int64\n",
      "2ndFlrSF         1460 non-null int64\n",
      "LowQualFinSF     1460 non-null int64\n",
      "GrLivArea        1460 non-null int64\n",
      "BsmtFullBath     1460 non-null int64\n",
      "BsmtHalfBath     1460 non-null int64\n",
      "FullBath         1460 non-null int64\n",
      "HalfBath         1460 non-null int64\n",
      "BedroomAbvGr     1460 non-null int64\n",
      "KitchenAbvGr     1460 non-null int64\n",
      "KitchenQual      1460 non-null object\n",
      "TotRmsAbvGrd     1460 non-null int64\n",
      "Functional       1460 non-null object\n",
      "Fireplaces       1460 non-null int64\n",
      "GarageType       1460 non-null object\n",
      "GarageYrBlt      1460 non-null float64\n",
      "GarageFinish     1460 non-null object\n",
      "GarageCars       1460 non-null int64\n",
      "GarageArea       1460 non-null int64\n",
      "GarageQual       1460 non-null object\n",
      "GarageCond       1460 non-null object\n",
      "PavedDrive       1460 non-null object\n",
      "WoodDeckSF       1460 non-null int64\n",
      "OpenPorchSF      1460 non-null int64\n",
      "EnclosedPorch    1460 non-null int64\n",
      "3SsnPorch        1460 non-null int64\n",
      "ScreenPorch      1460 non-null int64\n",
      "PoolArea         1460 non-null int64\n",
      "MiscVal          1460 non-null int64\n",
      "MoSold           1460 non-null int64\n",
      "YrSold           1460 non-null int64\n",
      "SaleType         1460 non-null object\n",
      "SaleCondition    1460 non-null object\n",
      "dtypes: float64(3), int64(33), object(38)\n",
      "memory usage: 844.1+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 74)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df= pd.read_csv('formulatedtest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=pd.concat([dataset,test_df],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 74)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns= ['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities','LotConfig','LandSlope','Neighborhood','Condition1',\n",
    "          'Condition2', 'BldgType','HouseStyle', 'RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','ExterQual',\n",
    "            'ExterCond','Foundation','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','Heating','HeatingQC',\n",
    "          'CentralAir','Electrical','KitchenQual','Functional','GarageType','GarageFinish','GarageQual','GarageCond',\n",
    "          'PavedDrive', 'SaleType','SaleCondition'\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_onehot_multcols(multcolumns):\n",
    "    df_final=final_df\n",
    "    i=0\n",
    "    for fields in multcolumns:\n",
    "        \n",
    "        print(fields)\n",
    "        df1=pd.get_dummies(final_df[fields],drop_first=True)\n",
    "        \n",
    "        final_df.drop([fields],axis=1,inplace=True)\n",
    "        if i==0:\n",
    "            df_final=df1.copy()\n",
    "        else:\n",
    "            \n",
    "            df_final=pd.concat([df_final,df1],axis=1)\n",
    "        i=i+1\n",
    "       \n",
    "        \n",
    "    df_final=pd.concat([final_df,df_final],axis=1)\n",
    "        \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df=final_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSZoning\n",
      "Street\n",
      "LotShape\n",
      "LandContour\n",
      "Utilities\n",
      "LotConfig\n",
      "LandSlope\n",
      "Neighborhood\n",
      "Condition1\n",
      "Condition2\n",
      "BldgType\n",
      "HouseStyle\n",
      "RoofStyle\n",
      "RoofMatl\n",
      "Exterior1st\n",
      "Exterior2nd\n",
      "MasVnrType\n",
      "ExterQual\n",
      "ExterCond\n",
      "Foundation\n",
      "BsmtQual\n",
      "BsmtCond\n",
      "BsmtExposure\n",
      "BsmtFinType1\n",
      "BsmtFinType2\n",
      "Heating\n",
      "HeatingQC\n",
      "CentralAir\n",
      "Electrical\n",
      "KitchenQual\n",
      "Functional\n",
      "GarageType\n",
      "GarageFinish\n",
      "GarageQual\n",
      "GarageCond\n",
      "PavedDrive\n",
      "SaleType\n",
      "SaleCondition\n"
     ]
    }
   ],
   "source": [
    "final_df=category_onehot_multcols(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df =final_df.loc[:,~final_df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>ConLI</th>\n",
       "      <th>ConLw</th>\n",
       "      <th>New</th>\n",
       "      <th>Oth</th>\n",
       "      <th>WD</th>\n",
       "      <th>AdjLand</th>\n",
       "      <th>Alloca</th>\n",
       "      <th>Family</th>\n",
       "      <th>Normal</th>\n",
       "      <th>Partial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 176 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0          60         65.0     8450            7            5       2003   \n",
       "1          20         80.0     9600            6            8       1976   \n",
       "2          60         68.0    11250            7            5       2001   \n",
       "3          70         60.0     9550            7            5       1915   \n",
       "4          60         84.0    14260            8            5       2000   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  ConLI  ConLw  New  \\\n",
       "0          2003       196.0       706.0         0.0  ...      0      0    0   \n",
       "1          1976         0.0       978.0         0.0  ...      0      0    0   \n",
       "2          2002       162.0       486.0         0.0  ...      0      0    0   \n",
       "3          1970         0.0       216.0         0.0  ...      0      0    0   \n",
       "4          2000       350.0       655.0         0.0  ...      0      0    0   \n",
       "\n",
       "   Oth  WD  AdjLand  Alloca  Family  Normal  Partial  \n",
       "0    0   1        0       0       0       1        0  \n",
       "1    0   1        0       0       0       1        0  \n",
       "2    0   1        0       0       0       1        0  \n",
       "3    0   1        0       0       0       0        0  \n",
       "4    0   1        0       0       0       1        0  \n",
       "\n",
       "[5 rows x 176 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train=final_df.iloc[:1460,:]\n",
    "df_Test=final_df.iloc[1460:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 1459)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_Train), len(df_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor= LinearRegression()\n",
    "model= regressor.fit(df_Train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(df_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "s= pd.read_csv('E:\\study material\\programming\\ML\\Datasets\\House_price Regression\\\\sample_submission.csv')\n",
    "\n",
    "submission= pd.DataFrame({\n",
    "    \"Id\": s['Id'],\n",
    "    \"SalePrice\": predictions\n",
    "})\n",
    "\n",
    "##submission.to_csv('submissionLR.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ML\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "reg_svr= SVR(C=10, kernel='rbf')\n",
    "model2= reg_svr.fit(df_Train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_svr= model.predict(df_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission2= pd.DataFrame({\n",
    "    \"Id\": s['Id'],\n",
    "    \"SalePrice\": pre_svr\n",
    "})\n",
    "\n",
    "submission2.to_csv('submission_svr.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 176)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU,PReLU,ELU\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ML\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=176, units=50, kernel_initializer=\"he_uniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "D:\\ML\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=25, kernel_initializer=\"he_uniform\")`\n",
      "  import sys\n",
      "D:\\ML\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=50, kernel_initializer=\"he_uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "D:\\ML\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1, kernel_initializer=\"he_uniform\")`\n",
      "  if sys.path[0] == '':\n",
      "D:\\ML\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ML\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1000\n",
      "1168/1168 [==============================] - 2s 2ms/step - loss: 22250225879.6712 - val_loss: 5635221240.9863\n",
      "Epoch 2/1000\n",
      "1168/1168 [==============================] - 0s 216us/step - loss: 6055485103.2329 - val_loss: 5095905659.1781\n",
      "Epoch 3/1000\n",
      "1168/1168 [==============================] - 0s 144us/step - loss: 5295149441.3151 - val_loss: 4828446401.3151\n",
      "Epoch 4/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 4746738100.4932 - val_loss: 4618434766.4658\n",
      "Epoch 5/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 4328059705.6438 - val_loss: 4426416191.1233\n",
      "Epoch 6/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 3919888071.8356 - val_loss: 4264013905.7534\n",
      "Epoch 7/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 3590760480.8219 - val_loss: 4092089971.5068\n",
      "Epoch 8/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 3284761962.9589 - val_loss: 3926625680.2192\n",
      "Epoch 9/1000\n",
      "1168/1168 [==============================] - 0s 161us/step - loss: 3018840500.2192 - val_loss: 3758299072.0000\n",
      "Epoch 10/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 2787990638.7397 - val_loss: 3621128530.4110\n",
      "Epoch 11/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 2545118144.6575 - val_loss: 3504737293.5890\n",
      "Epoch 12/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 2331174286.2466 - val_loss: 3382047662.2466\n",
      "Epoch 13/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 2144622084.6575 - val_loss: 3293790044.7123\n",
      "Epoch 14/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 1999530765.0959 - val_loss: 3303577946.9589\n",
      "Epoch 15/1000\n",
      "1168/1168 [==============================] - 0s 139us/step - loss: 1878435313.2603 - val_loss: 3379292527.1233\n",
      "Epoch 16/1000\n",
      "1168/1168 [==============================] - 0s 153us/step - loss: 1788967195.3151 - val_loss: 3298476942.2466\n",
      "Epoch 17/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 1679086242.4658 - val_loss: 3298643078.3562\n",
      "Epoch 18/1000\n",
      "1168/1168 [==============================] - 0s 195us/step - loss: 1644184207.6986 - val_loss: 3482102630.1370\n",
      "Epoch 19/1000\n",
      "1168/1168 [==============================] - 0s 201us/step - loss: 1615309657.0411 - val_loss: 3376230049.7534\n",
      "Epoch 20/1000\n",
      "1168/1168 [==============================] - 0s 219us/step - loss: 1590599144.7397 - val_loss: 3358160084.6027\n",
      "Epoch 21/1000\n",
      "1168/1168 [==============================] - 0s 190us/step - loss: 1598374961.7534 - val_loss: 3405003382.7945\n",
      "Epoch 22/1000\n",
      "1168/1168 [==============================] - 0s 191us/step - loss: 1570789108.6301 - val_loss: 3375239580.9315\n",
      "Epoch 23/1000\n",
      "1168/1168 [==============================] - 0s 160us/step - loss: 1580944692.4658 - val_loss: 3479541758.9041\n",
      "Epoch 24/1000\n",
      "1168/1168 [==============================] - 0s 161us/step - loss: 1574798694.5205 - val_loss: 3448043840.6575\n",
      "Epoch 25/1000\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 1575843700.6438 - val_loss: 3604829708.0548\n",
      "Epoch 26/1000\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 1574805874.3562 - val_loss: 3435206679.2329\n",
      "Epoch 27/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 1555263062.4110 - val_loss: 3607393356.7123\n",
      "Epoch 28/1000\n",
      "1168/1168 [==============================] - 0s 139us/step - loss: 1587815693.2877 - val_loss: 3395044300.2740\n",
      "Epoch 29/1000\n",
      "1168/1168 [==============================] - 0s 143us/step - loss: 1565743244.6575 - val_loss: 3379920541.8082\n",
      "Epoch 30/1000\n",
      "1168/1168 [==============================] - 0s 154us/step - loss: 1557372723.6027 - val_loss: 3362663566.4658\n",
      "Epoch 31/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 1562588753.2329 - val_loss: 3364783827.5068\n",
      "Epoch 32/1000\n",
      "1168/1168 [==============================] - 0s 271us/step - loss: 1575796165.6438 - val_loss: 3399088750.9041\n",
      "Epoch 33/1000\n",
      "1168/1168 [==============================] - 0s 196us/step - loss: 1566535247.1233 - val_loss: 3476355784.3288\n",
      "Epoch 34/1000\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 1542595760.0822 - val_loss: 3482248156.4932\n",
      "Epoch 35/1000\n",
      "1168/1168 [==============================] - 0s 148us/step - loss: 1564102983.1233 - val_loss: 3384998468.3836\n",
      "Epoch 36/1000\n",
      "1168/1168 [==============================] - 0s 151us/step - loss: 1545382941.2329 - val_loss: 3447018430.2466\n",
      "Epoch 37/1000\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 1537579771.2466 - val_loss: 3413487948.7123\n",
      "Epoch 38/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 1545126273.9589 - val_loss: 3305679863.2329\n",
      "Epoch 39/1000\n",
      "1168/1168 [==============================] - 0s 132us/step - loss: 1550195084.6027 - val_loss: 3321481465.8630\n",
      "Epoch 40/1000\n",
      "1168/1168 [==============================] - 0s 151us/step - loss: 1529595961.0685 - val_loss: 3532460112.8767\n",
      "Epoch 41/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 1545759820.5890 - val_loss: 3329063552.8767\n",
      "Epoch 42/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 1524499564.2466 - val_loss: 3383028373.0411\n",
      "Epoch 43/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 1539834716.5753 - val_loss: 3429887795.9452\n",
      "Epoch 44/1000\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 1529189825.2603 - val_loss: 3422997729.3151\n",
      "Epoch 45/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 1529883348.2740 - val_loss: 3317930903.8904\n",
      "Epoch 46/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 1528303790.6575 - val_loss: 3455694824.9863\n",
      "Epoch 47/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 1527286932.3973 - val_loss: 3347971375.5616\n",
      "Epoch 48/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 1537099248.3014 - val_loss: 3366033241.8630\n",
      "Epoch 49/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 1531779728.6301 - val_loss: 3438775300.6027\n",
      "Epoch 50/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 1534898117.9452 - val_loss: 3357404448.0000\n",
      "Epoch 51/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 1510221525.0000 - val_loss: 3393549246.6849\n",
      "Epoch 52/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 1513647036.1096 - val_loss: 3488668125.3699\n",
      "Epoch 53/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 1525497548.6301 - val_loss: 3300353367.0137\n",
      "Epoch 54/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 1504246748.8219 - val_loss: 3387819084.9315\n",
      "Epoch 55/1000\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 1508296402.3562 - val_loss: 3357103235.0685\n",
      "Epoch 56/1000\n",
      "1168/1168 [==============================] - 0s 148us/step - loss: 1491469968.0822 - val_loss: 3287279854.4658\n",
      "Epoch 57/1000\n",
      "1168/1168 [==============================] - 0s 150us/step - loss: 1503026702.8219 - val_loss: 3321019643.6164\n",
      "Epoch 58/1000\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 1505591318.2740 - val_loss: 3339355968.8767\n",
      "Epoch 59/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 1474209525.1507 - val_loss: 3323434535.2329\n",
      "Epoch 60/1000\n",
      "1168/1168 [==============================] - 0s 156us/step - loss: 1480263751.2603 - val_loss: 3304523918.0274\n",
      "Epoch 61/1000\n",
      "1168/1168 [==============================] - 0s 140us/step - loss: 1489386794.1644 - val_loss: 3390088551.2329\n",
      "Epoch 62/1000\n",
      "1168/1168 [==============================] - 0s 145us/step - loss: 1453509554.1370 - val_loss: 3249944220.7123\n",
      "Epoch 63/1000\n",
      "1168/1168 [==============================] - 0s 158us/step - loss: 1486069748.3836 - val_loss: 3293742874.3014\n",
      "Epoch 64/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 190us/step - loss: 1456534685.2603 - val_loss: 3210699260.7123\n",
      "Epoch 65/1000\n",
      "1168/1168 [==============================] - 0s 252us/step - loss: 1465441609.2329 - val_loss: 3238732323.9452\n",
      "Epoch 66/1000\n",
      "1168/1168 [==============================] - 0s 239us/step - loss: 1450069429.0411 - val_loss: 3233662644.8219\n",
      "Epoch 67/1000\n",
      "1168/1168 [==============================] - 0s 162us/step - loss: 1465129355.2329 - val_loss: 3262783475.9452\n",
      "Epoch 68/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 1464609980.6027 - val_loss: 3253061576.5479\n",
      "Epoch 69/1000\n",
      "1168/1168 [==============================] - 0s 137us/step - loss: 1450682807.8630 - val_loss: 3301340842.7397\n",
      "Epoch 70/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 1451976230.0000 - val_loss: 3288636435.2877\n",
      "Epoch 71/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 1440780439.0137 - val_loss: 3312215391.5616\n",
      "Epoch 72/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 1452111189.5890 - val_loss: 3306725233.0959\n",
      "Epoch 73/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 1456887991.9726 - val_loss: 3368192733.1507\n",
      "Epoch 74/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 1437097088.6301 - val_loss: 3235533234.8493\n",
      "Epoch 75/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1526874308.54 - 0s 120us/step - loss: 1420519162.6575 - val_loss: 3339523749.6986\n",
      "Epoch 76/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 1433057554.0822 - val_loss: 3291711798.1370\n",
      "Epoch 77/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 1426037867.8630 - val_loss: 3259760080.8767\n",
      "Epoch 78/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 1411327669.1849 - val_loss: 3362525858.1918\n",
      "Epoch 79/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 1416409772.7123 - val_loss: 3407520866.1918\n",
      "Epoch 80/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 1409982093.7260 - val_loss: 3288385039.3425\n",
      "Epoch 81/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 1409519677.6438 - val_loss: 3372779462.1370\n",
      "Epoch 82/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 1410191143.2877 - val_loss: 3263693005.3699\n",
      "Epoch 83/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 1405159913.0959 - val_loss: 3283447786.9589\n",
      "Epoch 84/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 1388225458.5890 - val_loss: 3422999784.7671\n",
      "Epoch 85/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 1392320146.9863 - val_loss: 3304583246.2466\n",
      "Epoch 86/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 1395050858.0137 - val_loss: 3364598632.9863\n",
      "Epoch 87/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 1376109518.9315 - val_loss: 3262750144.0000\n",
      "Epoch 88/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 1380280224.6301 - val_loss: 3236283203.5068\n",
      "Epoch 89/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 1392695623.2192 - val_loss: 3207089086.2466\n",
      "Epoch 90/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 1383386372.7671 - val_loss: 3241798130.6301\n",
      "Epoch 91/1000\n",
      "1168/1168 [==============================] - 0s 140us/step - loss: 1367962773.9452 - val_loss: 3226539749.0411\n",
      "Epoch 92/1000\n",
      "1168/1168 [==============================] - 0s 157us/step - loss: 1384516562.6164 - val_loss: 3250460822.5753\n",
      "Epoch 93/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 1377141729.8904 - val_loss: 3386323363.5068\n",
      "Epoch 94/1000\n",
      "1168/1168 [==============================] - 0s 154us/step - loss: 1372531296.4110 - val_loss: 3296558048.2192\n",
      "Epoch 95/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 1348730042.7397 - val_loss: 3367640782.0274\n",
      "Epoch 96/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 1353638707.4247 - val_loss: 3387935415.0137\n",
      "Epoch 97/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 1350272824.0822 - val_loss: 3254227685.4795\n",
      "Epoch 98/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 1347354674.8493 - val_loss: 3455277130.7397\n",
      "Epoch 99/1000\n",
      "1168/1168 [==============================] - 0s 161us/step - loss: 1354898060.5753 - val_loss: 3247178660.1644\n",
      "Epoch 100/1000\n",
      "1168/1168 [==============================] - 0s 231us/step - loss: 1352887726.3836 - val_loss: 3229364780.9315\n",
      "Epoch 101/1000\n",
      "1168/1168 [==============================] - 0s 251us/step - loss: 1332270891.0411 - val_loss: 3314093977.4247\n",
      "Epoch 102/1000\n",
      "1168/1168 [==============================] - 0s 202us/step - loss: 1342803050.4384 - val_loss: 3210837341.8082\n",
      "Epoch 103/1000\n",
      "1168/1168 [==============================] - 0s 188us/step - loss: 1350997729.5068 - val_loss: 3199785294.0274\n",
      "Epoch 104/1000\n",
      "1168/1168 [==============================] - 0s 139us/step - loss: 1347657484.0000 - val_loss: 3230451944.7671\n",
      "Epoch 105/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 1345471000.5753 - val_loss: 3211044573.1507\n",
      "Epoch 106/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 1339693664.4384 - val_loss: 3318071512.1096\n",
      "Epoch 107/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 1324083551.0411 - val_loss: 3180688585.2055\n",
      "Epoch 108/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 1316031847.6575 - val_loss: 3375501594.5205\n",
      "Epoch 109/1000\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 1327206695.8082 - val_loss: 3302577836.9315\n",
      "Epoch 110/1000\n",
      "1168/1168 [==============================] - 0s 146us/step - loss: 1312956161.9452 - val_loss: 3261113578.0822\n",
      "Epoch 111/1000\n",
      "1168/1168 [==============================] - 0s 163us/step - loss: 1307447317.8630 - val_loss: 3301014237.8082\n",
      "Epoch 112/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 1304454710.8767 - val_loss: 3279759091.9452\n",
      "Epoch 113/1000\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 1305198820.3836 - val_loss: 3201191043.5068\n",
      "Epoch 114/1000\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 1313028117.1164 - val_loss: 3180994564.3836\n",
      "Epoch 115/1000\n",
      "1168/1168 [==============================] - 0s 161us/step - loss: 1299044355.0411 - val_loss: 3166695282.4110\n",
      "Epoch 116/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 1305430042.8493 - val_loss: 3328790132.6027\n",
      "Epoch 117/1000\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 1302420117.8219 - val_loss: 3181259030.3562\n",
      "Epoch 118/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 1290921704.9795 - val_loss: 3280895016.5479\n",
      "Epoch 119/1000\n",
      "1168/1168 [==============================] - 0s 155us/step - loss: 1297527420.1370 - val_loss: 3246234489.6438\n",
      "Epoch 120/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 1277939293.2329 - val_loss: 3209612113.3151\n",
      "Epoch 121/1000\n",
      "1168/1168 [==============================] - 0s 137us/step - loss: 1275584727.9178 - val_loss: 3362686352.8767\n",
      "Epoch 122/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 1268467607.1370 - val_loss: 3186135726.4658\n",
      "Epoch 123/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 1284785076.0822 - val_loss: 3178892159.5616\n",
      "Epoch 124/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 1282957203.2603 - val_loss: 3089958734.4658\n",
      "Epoch 125/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 1275196995.6164 - val_loss: 3101838238.9041\n",
      "Epoch 126/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 1272886476.8767 - val_loss: 3165092085.6986\n",
      "Epoch 127/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 1265946076.4521 - val_loss: 3109355831.2329\n",
      "Epoch 128/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 1252342552.7808 - val_loss: 3071324535.8904\n",
      "Epoch 129/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 122us/step - loss: 1261704056.8219 - val_loss: 3121583194.7397\n",
      "Epoch 130/1000\n",
      "1168/1168 [==============================] - 0s 137us/step - loss: 1257831952.8219 - val_loss: 3156207458.8493\n",
      "Epoch 131/1000\n",
      "1168/1168 [==============================] - 0s 209us/step - loss: 1257807863.9315 - val_loss: 3132240009.6438\n",
      "Epoch 132/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 1258670528.8219 - val_loss: 3162901432.5479\n",
      "Epoch 133/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 1264681741.1644 - val_loss: 3134134543.5616\n",
      "Epoch 134/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 1243360752.5479 - val_loss: 3143381803.6164\n",
      "Epoch 135/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 1241993399.4384 - val_loss: 3255965609.2055\n",
      "Epoch 136/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 1228826139.6438 - val_loss: 3086708880.4384\n",
      "Epoch 137/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 1239257069.2877 - val_loss: 3041604537.6438\n",
      "Epoch 138/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 1220665846.9726 - val_loss: 3078042622.6849\n",
      "Epoch 139/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 1228572075.6712 - val_loss: 3151012860.7123\n",
      "Epoch 140/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 1216641108.6712 - val_loss: 2998347586.6301\n",
      "Epoch 141/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 1219440154.1096 - val_loss: 2998573622.3562\n",
      "Epoch 142/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 1231149828.6301 - val_loss: 3003005690.7397\n",
      "Epoch 143/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 1209297733.1781 - val_loss: 3037624087.0137\n",
      "Epoch 144/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 1209492900.6164 - val_loss: 3000165010.6301\n",
      "Epoch 145/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 1220730111.7534 - val_loss: 2951962208.0000\n",
      "Epoch 146/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 1218797276.1370 - val_loss: 2907055265.9726\n",
      "Epoch 147/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 1218279363.8493 - val_loss: 3026351101.5890\n",
      "Epoch 148/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 1203046235.9041 - val_loss: 3083385459.2877\n",
      "Epoch 149/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 1202366514.1233 - val_loss: 2946956646.5753\n",
      "Epoch 150/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 1209019565.9315 - val_loss: 2966718962.1918\n",
      "Epoch 151/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 1188310127.1370 - val_loss: 3029768860.9315\n",
      "Epoch 152/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 1192797269.9726 - val_loss: 3016069119.1233\n",
      "Epoch 153/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 1189983086.9178 - val_loss: 2889469049.4247\n",
      "Epoch 154/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 1176858892.9452 - val_loss: 2872666791.2329\n",
      "Epoch 155/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 1181423804.1507 - val_loss: 2952530512.8767\n",
      "Epoch 156/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 1175634243.7740 - val_loss: 2865330415.3425\n",
      "Epoch 157/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 1165509127.6438 - val_loss: 2918400650.3014\n",
      "Epoch 158/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 1176931628.7945 - val_loss: 2830793760.8767\n",
      "Epoch 159/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 1186589539.6986 - val_loss: 2911027723.6164\n",
      "Epoch 160/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 1159905935.0753 - val_loss: 2780279008.0000\n",
      "Epoch 161/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 1169703664.8493 - val_loss: 2774068267.3973\n",
      "Epoch 162/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 1154263414.8219 - val_loss: 2875790551.8904\n",
      "Epoch 163/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 1175855238.8082 - val_loss: 2789495134.6849\n",
      "Epoch 164/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 1157625450.5205 - val_loss: 2858425067.6164\n",
      "Epoch 165/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 1153086133.0959 - val_loss: 2742194547.7260\n",
      "Epoch 166/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 1164846271.5068 - val_loss: 2809004300.9315\n",
      "Epoch 167/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 1143703675.6164 - val_loss: 2833042472.3288\n",
      "Epoch 168/1000\n",
      "1168/1168 [==============================] - 0s 241us/step - loss: 1138289349.9863 - val_loss: 2840488264.5479\n",
      "Epoch 169/1000\n",
      "1168/1168 [==============================] - 0s 207us/step - loss: 1167805221.8493 - val_loss: 2805620862.4658\n",
      "Epoch 170/1000\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 1141658191.8630 - val_loss: 2823498470.7945\n",
      "Epoch 171/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 1138284586.6849 - val_loss: 2778463758.9041\n",
      "Epoch 172/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 1134379470.1096 - val_loss: 2757814681.6438\n",
      "Epoch 173/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 1133640282.1096 - val_loss: 2761759937.0959\n",
      "Epoch 174/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 1128234108.3699 - val_loss: 2763703866.0822\n",
      "Epoch 175/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 1133260365.1644 - val_loss: 2861720030.2466\n",
      "Epoch 176/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 1124228867.2329 - val_loss: 2754616941.6986\n",
      "Epoch 177/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 1111921153.9726 - val_loss: 2679716649.6438\n",
      "Epoch 178/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 1121504495.2877 - val_loss: 2731023711.7808\n",
      "Epoch 179/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 1126370787.2192 - val_loss: 2739922052.8219\n",
      "Epoch 180/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 1113461552.0411 - val_loss: 2709153392.2192\n",
      "Epoch 181/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 1110587424.0548 - val_loss: 2666340035.7260\n",
      "Epoch 182/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 1103926000.5068 - val_loss: 2747701674.5205\n",
      "Epoch 183/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 1094195678.1096 - val_loss: 2689476867.5068\n",
      "Epoch 184/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 1100984176.8219 - val_loss: 2828195351.3425\n",
      "Epoch 185/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 1096150055.6438 - val_loss: 2699346428.9315\n",
      "Epoch 186/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 1098728886.7808 - val_loss: 2687967234.4110\n",
      "Epoch 187/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 1092480389.5616 - val_loss: 2712865707.9452\n",
      "Epoch 188/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 1105125603.86 - 0s 126us/step - loss: 1080963442.3425 - val_loss: 2708276706.1918\n",
      "Epoch 189/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 1087706161.9041 - val_loss: 2644890207.3425\n",
      "Epoch 190/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 1096909744.4932 - val_loss: 2594099158.5753\n",
      "Epoch 191/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 1096681466.6575 - val_loss: 2639240718.9041\n",
      "Epoch 192/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 1111509837.3288 - val_loss: 2715425423.8904\n",
      "Epoch 193/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 1080187145.3562 - val_loss: 2594874865.3151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 1070770579.3836 - val_loss: 2717973754.1918\n",
      "Epoch 195/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 1075778230.6507 - val_loss: 2761645089.0959\n",
      "Epoch 196/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 1069471303.8082 - val_loss: 2633246393.4247\n",
      "Epoch 197/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 1063631919.1370 - val_loss: 2640190282.9589\n",
      "Epoch 198/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 1064454846.1507 - val_loss: 2703851759.7808\n",
      "Epoch 199/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 1102131160.4658 - val_loss: 2592048048.9863\n",
      "Epoch 200/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 1055381168.7808 - val_loss: 2607071592.2192\n",
      "Epoch 201/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 1084758474.8082 - val_loss: 2679088365.8082\n",
      "Epoch 202/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 1049858834.0822 - val_loss: 2522028612.1644\n",
      "Epoch 203/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 1055495192.8219 - val_loss: 2671915417.9726\n",
      "Epoch 204/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 1074975305.8630 - val_loss: 2536788273.5342\n",
      "Epoch 205/1000\n",
      "1168/1168 [==============================] - 0s 205us/step - loss: 1056693472.2192 - val_loss: 2553052143.1233\n",
      "Epoch 206/1000\n",
      "1168/1168 [==============================] - 0s 213us/step - loss: 1053629197.7260 - val_loss: 2585888867.2877\n",
      "Epoch 207/1000\n",
      "1168/1168 [==============================] - 0s 138us/step - loss: 1061457386.0548 - val_loss: 2589470063.8904\n",
      "Epoch 208/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 1029023558.4247 - val_loss: 2540714290.4110\n",
      "Epoch 209/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 1058308848.7192 - val_loss: 2647694989.3699\n",
      "Epoch 210/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 1035216578.5342 - val_loss: 2548586637.5890\n",
      "Epoch 211/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 1032489334.3014 - val_loss: 2501596588.4932\n",
      "Epoch 212/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 1033676453.5479 - val_loss: 2555954075.2877\n",
      "Epoch 213/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 1043522795.6849 - val_loss: 2544294948.7123\n",
      "Epoch 214/1000\n",
      "1168/1168 [==============================] - 0s 153us/step - loss: 1047021759.1644 - val_loss: 2512395387.1781\n",
      "Epoch 215/1000\n",
      "1168/1168 [==============================] - 0s 153us/step - loss: 1040548699.6575 - val_loss: 2619970973.6986\n",
      "Epoch 216/1000\n",
      "1168/1168 [==============================] - 0s 154us/step - loss: 1028632042.7260 - val_loss: 2467475311.7808\n",
      "Epoch 217/1000\n",
      "1168/1168 [==============================] - 0s 146us/step - loss: 1025305368.3562 - val_loss: 2482189996.1644\n",
      "Epoch 218/1000\n",
      "1168/1168 [==============================] - 0s 157us/step - loss: 1012731282.8219 - val_loss: 2428457555.3973\n",
      "Epoch 219/1000\n",
      "1168/1168 [==============================] - 0s 146us/step - loss: 1006209403.9452 - val_loss: 2412881456.6575\n",
      "Epoch 220/1000\n",
      "1168/1168 [==============================] - 0s 139us/step - loss: 1019046222.8767 - val_loss: 2397227883.6164\n",
      "Epoch 221/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 1012246971.8630 - val_loss: 2494040520.6575\n",
      "Epoch 222/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 1009854965.2466 - val_loss: 2466379190.7945\n",
      "Epoch 223/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 1014950312.7397 - val_loss: 2447341634.0822\n",
      "Epoch 224/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 984847614.4521 - val_loss: 2546827885.4247\n",
      "Epoch 225/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 1005268877.2877 - val_loss: 2457164811.5068\n",
      "Epoch 226/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 991453259.3288 - val_loss: 2371801579.7808\n",
      "Epoch 227/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 996179218.9041 - val_loss: 2477993521.3151\n",
      "Epoch 228/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 997740919.1507 - val_loss: 2398622201.5342\n",
      "Epoch 229/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 985964880.1233 - val_loss: 2592596551.1233\n",
      "Epoch 230/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 1004075662.5616 - val_loss: 2434540583.4521\n",
      "Epoch 231/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 983968207.3973 - val_loss: 2376308585.8630\n",
      "Epoch 232/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 988256748.9932 - val_loss: 2356474734.1370\n",
      "Epoch 233/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 997849039.0274 - val_loss: 2381487273.6986\n",
      "Epoch 234/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 985345911.0205 - val_loss: 2315976806.9589\n",
      "Epoch 235/1000\n",
      "1168/1168 [==============================] - 0s 145us/step - loss: 962669136.4521 - val_loss: 2407361791.1233\n",
      "Epoch 236/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 983982617.2877 - val_loss: 2350458500.2192\n",
      "Epoch 237/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 947568839.8767 - val_loss: 2290360534.6849\n",
      "Epoch 238/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 977872929.7397 - val_loss: 2400566742.9041\n",
      "Epoch 239/1000\n",
      "1168/1168 [==============================] - 0s 136us/step - loss: 968918258.3562 - val_loss: 2292409147.7260\n",
      "Epoch 240/1000\n",
      "1168/1168 [==============================] - 0s 211us/step - loss: 969801401.1096 - val_loss: 2341269657.2603\n",
      "Epoch 241/1000\n",
      "1168/1168 [==============================] - 0s 190us/step - loss: 952395479.7055 - val_loss: 2351061946.6301\n",
      "Epoch 242/1000\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 972217068.0274 - val_loss: 2367071361.5342\n",
      "Epoch 243/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 945472817.0411 - val_loss: 2273032676.7123\n",
      "Epoch 244/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 954071692.8288 - val_loss: 2198672546.5205\n",
      "Epoch 245/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 953027748.9041 - val_loss: 2300798898.7397\n",
      "Epoch 246/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 950365677.1096 - val_loss: 2247558927.8904\n",
      "Epoch 247/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 929505149.2740 - val_loss: 2209895072.5479\n",
      "Epoch 248/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 933947617.3699 - val_loss: 2318267893.2603\n",
      "Epoch 249/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 952210130.6575 - val_loss: 2447580859.1781\n",
      "Epoch 250/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 929159615.5616 - val_loss: 2163182276.4932\n",
      "Epoch 251/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 944588611.8767 - val_loss: 2269367554.0822\n",
      "Epoch 252/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 927013871.8082 - val_loss: 2435490267.8356\n",
      "Epoch 253/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 951203667.0411 - val_loss: 2205393438.7945\n",
      "Epoch 254/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 918807863.7671 - val_loss: 2100812926.3562\n",
      "Epoch 255/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 910624656.3288 - val_loss: 2102754982.0274\n",
      "Epoch 256/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 938056626.9041 - val_loss: 2196849138.7397\n",
      "Epoch 257/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 902888880.3151 - val_loss: 2265979739.5068\n",
      "Epoch 258/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 909901505.1986 - val_loss: 2270729378.0822\n",
      "Epoch 259/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 123us/step - loss: 903085594.3836 - val_loss: 2260666540.0548\n",
      "Epoch 260/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 908633182.0685 - val_loss: 2139121634.2466\n",
      "Epoch 261/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 925581096.6027 - val_loss: 2163029460.6027\n",
      "Epoch 262/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 958860453.1918 - val_loss: 2212241480.2192\n",
      "Epoch 263/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 905885418.1096 - val_loss: 2307487357.3699\n",
      "Epoch 264/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 916709948.9315 - val_loss: 2223766296.3288\n",
      "Epoch 265/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 896255955.3151 - val_loss: 2235126094.0274\n",
      "Epoch 266/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 891207691.7671 - val_loss: 2139511255.7808\n",
      "Epoch 267/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 897018677.5137 - val_loss: 2085507398.3562\n",
      "Epoch 268/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 887050983.0822 - val_loss: 2227059971.0685\n",
      "Epoch 269/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 891545592.9178 - val_loss: 2101933574.8493\n",
      "Epoch 270/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 895087938.6301 - val_loss: 2041883852.6027\n",
      "Epoch 271/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 866621093.9315 - val_loss: 2162315963.5068\n",
      "Epoch 272/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 868778857.0959 - val_loss: 2024529252.0548\n",
      "Epoch 273/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 888760066.9041 - val_loss: 2100444360.1096\n",
      "Epoch 274/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 861280574.7123 - val_loss: 2100946625.0959\n",
      "Epoch 275/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 859561938.0548 - val_loss: 2074741691.6164\n",
      "Epoch 276/1000\n",
      "1168/1168 [==============================] - 0s 146us/step - loss: 858569578.7123 - val_loss: 1974480641.2055\n",
      "Epoch 277/1000\n",
      "1168/1168 [==============================] - 0s 213us/step - loss: 886201531.5068 - val_loss: 1973151243.6164\n",
      "Epoch 278/1000\n",
      "1168/1168 [==============================] - 0s 167us/step - loss: 855857750.7397 - val_loss: 2013339700.7123\n",
      "Epoch 279/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 855548840.0274 - val_loss: 2090685141.3699\n",
      "Epoch 280/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 865810890.0685 - val_loss: 2096030200.9863\n",
      "Epoch 281/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 853618652.2329 - val_loss: 1940694662.8493\n",
      "Epoch 282/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 865905471.4384 - val_loss: 1974789075.9452\n",
      "Epoch 283/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 839131887.8630 - val_loss: 2062091352.2192\n",
      "Epoch 284/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 844919229.9589 - val_loss: 1953845604.9315\n",
      "Epoch 285/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 835094822.7945 - val_loss: 1897890768.9863\n",
      "Epoch 286/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 867098926.5205 - val_loss: 2022904377.6438\n",
      "Epoch 287/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 862919220.4829 - val_loss: 1999431943.4521\n",
      "Epoch 288/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 838529528.8493 - val_loss: 1962181435.8356\n",
      "Epoch 289/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 842524859.2877 - val_loss: 2141075149.3699\n",
      "Epoch 290/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 837556061.0753 - val_loss: 1992588333.1507\n",
      "Epoch 291/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 843069770.6027 - val_loss: 1965330703.1233\n",
      "Epoch 292/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 820732610.6027 - val_loss: 1871171875.5068\n",
      "Epoch 293/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 829863000.6027 - val_loss: 1870175751.3425\n",
      "Epoch 294/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 810368229.7808 - val_loss: 1912937736.9863\n",
      "Epoch 295/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 808371790.9726 - val_loss: 1856609107.6164\n",
      "Epoch 296/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 800169206.3014 - val_loss: 1973013392.6575\n",
      "Epoch 297/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 809011785.4658 - val_loss: 1834648837.9178\n",
      "Epoch 298/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 804388766.7397 - val_loss: 1853419972.2740\n",
      "Epoch 299/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 795111992.5890 - val_loss: 1897835756.6575\n",
      "Epoch 300/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 811118694.3425 - val_loss: 1875117261.5890\n",
      "Epoch 301/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 796832298.0068 - val_loss: 1746374492.2740\n",
      "Epoch 302/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 813719813.030 - 0s 120us/step - loss: 801966006.3973 - val_loss: 1837655078.1370\n",
      "Epoch 303/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 809588862.0959 - val_loss: 1876618354.8493\n",
      "Epoch 304/1000\n",
      "1168/1168 [==============================] - 0s 141us/step - loss: 795651305.8356 - val_loss: 1769937543.6712\n",
      "Epoch 305/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 781082307.8356 - val_loss: 1853028992.6575\n",
      "Epoch 306/1000\n",
      "1168/1168 [==============================] - 0s 109us/step - loss: 780129579.1849 - val_loss: 1888853440.0000\n",
      "Epoch 307/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 779935464.7945 - val_loss: 1717859877.0411\n",
      "Epoch 308/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 777469959.4247 - val_loss: 1896527808.0000\n",
      "Epoch 309/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 767144341.0959 - val_loss: 1954023984.0000\n",
      "Epoch 310/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 789952293.7808 - val_loss: 1765064003.2877\n",
      "Epoch 311/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 780338719.9041 - val_loss: 1736930293.8082\n",
      "Epoch 312/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 760572901.4589 - val_loss: 1786801806.9041\n",
      "Epoch 313/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 764958427.1781 - val_loss: 1728943857.9726\n",
      "Epoch 314/1000\n",
      "1168/1168 [==============================] - 0s 213us/step - loss: 779351831.1233 - val_loss: 1812231625.7534\n",
      "Epoch 315/1000\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 761754352.3014 - val_loss: 1858737852.0548\n",
      "Epoch 316/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 742949579.7945 - val_loss: 1727884044.2740\n",
      "Epoch 317/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 770158540.5753 - val_loss: 1700331487.5616\n",
      "Epoch 318/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 747228703.8904 - val_loss: 1674666534.3562\n",
      "Epoch 319/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 746826273.8493 - val_loss: 1656146651.2877\n",
      "Epoch 320/1000\n",
      "1168/1168 [==============================] - 0s 151us/step - loss: 757056051.5753 - val_loss: 1663936231.7808\n",
      "Epoch 321/1000\n",
      "1168/1168 [==============================] - 0s 141us/step - loss: 739706109.6027 - val_loss: 1664689073.2055\n",
      "Epoch 322/1000\n",
      "1168/1168 [==============================] - 0s 156us/step - loss: 727332175.7192 - val_loss: 1724281421.9726\n",
      "Epoch 323/1000\n",
      "1168/1168 [==============================] - 0s 153us/step - loss: 758850613.2329 - val_loss: 1650218227.1781\n",
      "Epoch 324/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 155us/step - loss: 733228686.4521 - val_loss: 1586622121.3151\n",
      "Epoch 325/1000\n",
      "1168/1168 [==============================] - 0s 150us/step - loss: 733189442.2466 - val_loss: 1637724182.4658\n",
      "Epoch 326/1000\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 741044447.9726 - val_loss: 1616463967.6712\n",
      "Epoch 327/1000\n",
      "1168/1168 [==============================] - 0s 138us/step - loss: 762841896.1918 - val_loss: 1550996544.0000\n",
      "Epoch 328/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 726815516.5890 - val_loss: 1686032382.0274\n",
      "Epoch 329/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 707735123.0411 - val_loss: 1589909705.6438\n",
      "Epoch 330/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 736481583.1507 - val_loss: 1600100970.9589\n",
      "Epoch 331/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 728870864.0685 - val_loss: 1606782912.6575\n",
      "Epoch 332/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 714259046.7671 - val_loss: 1586516264.8767\n",
      "Epoch 333/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 704093265.2055 - val_loss: 1588505833.4247\n",
      "Epoch 334/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 706673767.7945 - val_loss: 1529709572.6027\n",
      "Epoch 335/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 691786515.2466 - val_loss: 1684980409.8630\n",
      "Epoch 336/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 712096597.0685 - val_loss: 1740385190.1370\n",
      "Epoch 337/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 698768327.7534 - val_loss: 1558583074.4110\n",
      "Epoch 338/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 696319595.7534 - val_loss: 1531434538.5205\n",
      "Epoch 339/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 685327842.1986 - val_loss: 1517558267.3973\n",
      "Epoch 340/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 701900429.8767 - val_loss: 1490583212.4932\n",
      "Epoch 341/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 724614482.2911 - val_loss: 1454107411.2877\n",
      "Epoch 342/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 727782695.2329 - val_loss: 1487735653.9178\n",
      "Epoch 343/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 693109106.2329 - val_loss: 1558374207.4521\n",
      "Epoch 344/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 671509876.6781 - val_loss: 1523827779.9452\n",
      "Epoch 345/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 709391195.0274 - val_loss: 1434677762.3014\n",
      "Epoch 346/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 680095526.0685 - val_loss: 1467786631.1781\n",
      "Epoch 347/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 705852895.0548 - val_loss: 1587902757.0411\n",
      "Epoch 348/1000\n",
      "1168/1168 [==============================] - 0s 163us/step - loss: 664220637.3425 - val_loss: 1444824704.1096\n",
      "Epoch 349/1000\n",
      "1168/1168 [==============================] - 0s 219us/step - loss: 683199465.8185 - val_loss: 1489077697.3151\n",
      "Epoch 350/1000\n",
      "1168/1168 [==============================] - 0s 167us/step - loss: 679717406.4658 - val_loss: 1473202859.5068\n",
      "Epoch 351/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 695714745.0274 - val_loss: 1491874226.4110\n",
      "Epoch 352/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 670018602.1233 - val_loss: 1433056386.3014\n",
      "Epoch 353/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 664102000.1849 - val_loss: 1415039645.1507\n",
      "Epoch 354/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 686515301.2466 - val_loss: 1408441572.7123\n",
      "Epoch 355/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 655990453.5548 - val_loss: 1478629568.4384\n",
      "Epoch 356/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 672914443.6301 - val_loss: 1438552569.7534\n",
      "Epoch 357/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 650379020.7945 - val_loss: 1374335909.1507\n",
      "Epoch 358/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 678825397.4795 - val_loss: 1419268358.9041\n",
      "Epoch 359/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 669338410.8630 - val_loss: 1371566560.4384\n",
      "Epoch 360/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 648291548.3904 - val_loss: 1422737230.9041\n",
      "Epoch 361/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 654800613.8082 - val_loss: 1412812070.7945\n",
      "Epoch 362/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 651400886.0959 - val_loss: 1470492876.6027\n",
      "Epoch 363/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 643035518.0137 - val_loss: 1412885018.7397\n",
      "Epoch 364/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 646288312.5616 - val_loss: 1311667541.4795\n",
      "Epoch 365/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 686254837.5068 - val_loss: 1320301350.3562\n",
      "Epoch 366/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 645083092.0685 - val_loss: 1390819006.7945\n",
      "Epoch 367/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 641891166.1438 - val_loss: 1336438026.3014\n",
      "Epoch 368/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 645600091.5548 - val_loss: 1357915132.3288\n",
      "Epoch 369/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 658456575.6027 - val_loss: 1389502967.3425\n",
      "Epoch 370/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 639035652.1096 - val_loss: 1399149757.3699\n",
      "Epoch 371/1000\n",
      "1168/1168 [==============================] - 0s 140us/step - loss: 622296576.6849 - val_loss: 1468449508.1644\n",
      "Epoch 372/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 669401911.5479 - val_loss: 1496685202.0822\n",
      "Epoch 373/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 641794084.3425 - val_loss: 1372404931.6164\n",
      "Epoch 374/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 643064654.8082 - val_loss: 1327499564.3836\n",
      "Epoch 375/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 663618982.5548 - val_loss: 1287906396.9315\n",
      "Epoch 376/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 618122321.3288 - val_loss: 1312225767.3425\n",
      "Epoch 377/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 630010345.9521 - val_loss: 1364163331.0685\n",
      "Epoch 378/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 612480001.8082 - val_loss: 1342814762.3014\n",
      "Epoch 379/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 623398383.5479 - val_loss: 1297837911.4521\n",
      "Epoch 380/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 652570312.7260 - val_loss: 1307521349.8082\n",
      "Epoch 381/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 625421744.0411 - val_loss: 1307086490.3014\n",
      "Epoch 382/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 673913846.8630 - val_loss: 1283810320.6575\n",
      "Epoch 383/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 589834574.7671 - val_loss: 1282939671.0137\n",
      "Epoch 384/1000\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 616391358.5959 - val_loss: 1277410241.7534\n",
      "Epoch 385/1000\n",
      "1168/1168 [==============================] - 0s 199us/step - loss: 653753819.7808 - val_loss: 1271118320.8767\n",
      "Epoch 386/1000\n",
      "1168/1168 [==============================] - 0s 198us/step - loss: 632251473.6027 - val_loss: 1258630011.3973\n",
      "Epoch 387/1000\n",
      "1168/1168 [==============================] - 0s 150us/step - loss: 615967838.1233 - val_loss: 1248574292.2740\n",
      "Epoch 388/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 611817915.0822 - val_loss: 1248267328.0000\n",
      "Epoch 389/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 120us/step - loss: 632250971.7397 - val_loss: 1274867912.6027\n",
      "Epoch 390/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 609024493.2329 - val_loss: 1269434234.4658\n",
      "Epoch 391/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 592331708.8493 - val_loss: 1272386662.0274\n",
      "Epoch 392/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 631380928.5068 - val_loss: 1266149324.7123\n",
      "Epoch 393/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 609636604.6712 - val_loss: 1258647682.6301\n",
      "Epoch 394/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 589613984.3904 - val_loss: 1232185354.1918\n",
      "Epoch 395/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 636418361.9452 - val_loss: 1244133053.4795\n",
      "Epoch 396/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 604873859.0000 - val_loss: 1256855261.3699\n",
      "Epoch 397/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 596871428.9932 - val_loss: 1257113242.0822\n",
      "Epoch 398/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 627504426.9452 - val_loss: 1239965543.6712\n",
      "Epoch 399/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 597791772.3151 - val_loss: 1268730039.5616\n",
      "Epoch 400/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 617393272.1712 - val_loss: 1213266885.6986\n",
      "Epoch 401/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 589206229.4110 - val_loss: 1308215687.6164\n",
      "Epoch 402/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 577548653.0411 - val_loss: 1420648520.7671\n",
      "Epoch 403/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 612693734.2740 - val_loss: 1189179341.2603\n",
      "Epoch 404/1000\n",
      "1168/1168 [==============================] - 0s 144us/step - loss: 579173091.3425 - val_loss: 1367364711.6712\n",
      "Epoch 405/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 690448380.0411 - val_loss: 1212150019.5068\n",
      "Epoch 406/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 626291923.0411 - val_loss: 1221956591.6712\n",
      "Epoch 407/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 611308976.5753 - val_loss: 1232333448.1096\n",
      "Epoch 408/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 586263738.9041 - val_loss: 1274580732.9315\n",
      "Epoch 409/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 629109429.0548 - val_loss: 1275577662.2466\n",
      "Epoch 410/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 563438827.1301 - val_loss: 1179364721.5342\n",
      "Epoch 411/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 604958409.1644 - val_loss: 1223000297.6438\n",
      "Epoch 412/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 567365866.6781 - val_loss: 1204866152.5479\n",
      "Epoch 413/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 561701944.1096 - val_loss: 1170806554.4110\n",
      "Epoch 414/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 588488964.6438 - val_loss: 1245283672.7671\n",
      "Epoch 415/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 566960230.3973 - val_loss: 1193896517.9178\n",
      "Epoch 416/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 584180475.8151 - val_loss: 1177745008.4384\n",
      "Epoch 417/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 571530267.5342 - val_loss: 1216836183.7808\n",
      "Epoch 418/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 564556206.9041 - val_loss: 1221408569.5342\n",
      "Epoch 419/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 580231186.6575 - val_loss: 1232348176.7123\n",
      "Epoch 420/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 593724016.8082 - val_loss: 1213534431.1233\n",
      "Epoch 421/1000\n",
      "1168/1168 [==============================] - 0s 149us/step - loss: 564208686.1712 - val_loss: 1205445464.5479\n",
      "Epoch 422/1000\n",
      "1168/1168 [==============================] - 0s 208us/step - loss: 570077114.6301 - val_loss: 1292031219.2877\n",
      "Epoch 423/1000\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 562907256.8973 - val_loss: 1160857778.8493\n",
      "Epoch 424/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 570755822.1644 - val_loss: 1178486774.7945\n",
      "Epoch 425/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 630084759.8493 - val_loss: 1183806025.2055\n",
      "Epoch 426/1000\n",
      "1168/1168 [==============================] - 0s 144us/step - loss: 582681222.5753 - val_loss: 1235390416.8767\n",
      "Epoch 427/1000\n",
      "1168/1168 [==============================] - 0s 138us/step - loss: 550307947.0137 - val_loss: 1164409973.5890\n",
      "Epoch 428/1000\n",
      "1168/1168 [==============================] - 0s 151us/step - loss: 553867882.7260 - val_loss: 1230224167.1233\n",
      "Epoch 429/1000\n",
      "1168/1168 [==============================] - 0s 153us/step - loss: 555625356.0822 - val_loss: 1196985012.0548\n",
      "Epoch 430/1000\n",
      "1168/1168 [==============================] - 0s 152us/step - loss: 532227619.9760 - val_loss: 1155605843.7260\n",
      "Epoch 431/1000\n",
      "1168/1168 [==============================] - 0s 144us/step - loss: 566438022.0959 - val_loss: 1217318312.3288\n",
      "Epoch 432/1000\n",
      "1168/1168 [==============================] - 0s 138us/step - loss: 565730726.4795 - val_loss: 1157717802.4110\n",
      "Epoch 433/1000\n",
      "1168/1168 [==============================] - 0s 137us/step - loss: 535412326.8219 - val_loss: 1175697458.3014\n",
      "Epoch 434/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 539283556.7123 - val_loss: 1153557987.3973\n",
      "Epoch 435/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 550855504.6027 - val_loss: 1162231907.2877\n",
      "Epoch 436/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 576329051.7260 - val_loss: 1203921478.3562\n",
      "Epoch 437/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 542222631.3425 - val_loss: 1197482135.4521\n",
      "Epoch 438/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 556937645.3151 - val_loss: 1156604127.7808\n",
      "Epoch 439/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 560305869.9658 - val_loss: 1502266957.8082\n",
      "Epoch 440/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 585040279.1370 - val_loss: 1472756075.1918\n",
      "Epoch 441/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 560979915.8630 - val_loss: 1233608254.9041\n",
      "Epoch 442/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 568908439.0959 - val_loss: 1201607269.5890\n",
      "Epoch 443/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 563094484.9178 - val_loss: 1271104302.4658\n",
      "Epoch 444/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 555296195.9726 - val_loss: 1177146154.7397\n",
      "Epoch 445/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 536481679.5068 - val_loss: 1160767425.0959\n",
      "Epoch 446/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 527022546.3973 - val_loss: 1178058166.1370\n",
      "Epoch 447/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 554276895.1507 - val_loss: 1174773045.6986\n",
      "Epoch 448/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 539652784.8767 - val_loss: 1155844335.3425\n",
      "Epoch 449/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 514336048.0822 - val_loss: 1149223538.9589\n",
      "Epoch 450/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 543689700.2603 - val_loss: 1155357598.9041\n",
      "Epoch 451/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 528830391.1233 - val_loss: 1260875724.0548\n",
      "Epoch 452/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 519177836.5205 - val_loss: 1169866361.2055\n",
      "Epoch 453/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 548004590.5890 - val_loss: 1156538518.7945\n",
      "Epoch 454/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 115us/step - loss: 509249362.2808 - val_loss: 1196293704.9863\n",
      "Epoch 455/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 547433745.0000 - val_loss: 1150728724.4932\n",
      "Epoch 456/1000\n",
      "1168/1168 [==============================] - 0s 175us/step - loss: 543872590.9315 - val_loss: 1167330212.6027\n",
      "Epoch 457/1000\n",
      "1168/1168 [==============================] - 0s 208us/step - loss: 513638698.5479 - val_loss: 1148893964.9315\n",
      "Epoch 458/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 522828451.1438 - val_loss: 1362195557.4795\n",
      "Epoch 459/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 542839369.6301 - val_loss: 1156909716.8219\n",
      "Epoch 460/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 518961402.7260 - val_loss: 1130693148.7123\n",
      "Epoch 461/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 505600858.3288 - val_loss: 1147149483.5068\n",
      "Epoch 462/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 506891917.3014 - val_loss: 1199792934.4658\n",
      "Epoch 463/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 529809570.3904 - val_loss: 1208684724.3836\n",
      "Epoch 464/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 520852180.9795 - val_loss: 1286341164.0548\n",
      "Epoch 465/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 521473488.8630 - val_loss: 1195355737.6438\n",
      "Epoch 466/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 536310958.7260 - val_loss: 1155757159.7808\n",
      "Epoch 467/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 497720628.4384 - val_loss: 1288970687.8630\n",
      "Epoch 468/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 532167727.2466 - val_loss: 1200331016.2192\n",
      "Epoch 469/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 515378862.9658 - val_loss: 1139546161.8630\n",
      "Epoch 470/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 547712722.1918 - val_loss: 1148979268.4932\n",
      "Epoch 471/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 508136800.8425 - val_loss: 1184767663.1233\n",
      "Epoch 472/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 494684795.8425 - val_loss: 1191149873.0959\n",
      "Epoch 473/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 524519035.8904 - val_loss: 1206454671.1233\n",
      "Epoch 474/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 514452410.1507 - val_loss: 1155515038.6849\n",
      "Epoch 475/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 501307797.1644 - val_loss: 1170700513.8630\n",
      "Epoch 476/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 492797075.959 - 0s 118us/step - loss: 489111314.0822 - val_loss: 1181721413.2603\n",
      "Epoch 477/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 546047054.9041 - val_loss: 1269443256.8767\n",
      "Epoch 478/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 521002053.4521 - val_loss: 1237656893.6986\n",
      "Epoch 479/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 488884663.4247 - val_loss: 1154756220.7123\n",
      "Epoch 480/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 513512700.1301 - val_loss: 1133898318.6849\n",
      "Epoch 481/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 541965559.5753 - val_loss: 1248105188.7123\n",
      "Epoch 482/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 522082117.4795 - val_loss: 1230564432.9863\n",
      "Epoch 483/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 513556020.0411 - val_loss: 1264587584.5479\n",
      "Epoch 484/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 521270317.7808 - val_loss: 1276899193.4247\n",
      "Epoch 485/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 516555007.3082 - val_loss: 1166706452.2740\n",
      "Epoch 486/1000\n",
      "1168/1168 [==============================] - 0s 142us/step - loss: 489211351.4384 - val_loss: 1193104734.1370\n",
      "Epoch 487/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 500285494.5616 - val_loss: 1298680330.1918\n",
      "Epoch 488/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 508322442.0411 - val_loss: 1223618621.3699\n",
      "Epoch 489/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 498500866.5822 - val_loss: 1167043058.6301\n",
      "Epoch 490/1000\n",
      "1168/1168 [==============================] - 0s 136us/step - loss: 504511102.8493 - val_loss: 1291355961.7534\n",
      "Epoch 491/1000\n",
      "1168/1168 [==============================] - 0s 158us/step - loss: 492107650.9658 - val_loss: 1217871691.8904\n",
      "Epoch 492/1000\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 537443413.5753 - val_loss: 1278528071.8904\n",
      "Epoch 493/1000\n",
      "1168/1168 [==============================] - 0s 227us/step - loss: 478617072.7534 - val_loss: 1199387043.8356\n",
      "Epoch 494/1000\n",
      "1168/1168 [==============================] - 0s 216us/step - loss: 480585967.7808 - val_loss: 1197192596.0548\n",
      "Epoch 495/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 496034128.7671 - val_loss: 1216788129.8630\n",
      "Epoch 496/1000\n",
      "1168/1168 [==============================] - 0s 143us/step - loss: 525185818.7945 - val_loss: 1190380674.0822\n",
      "Epoch 497/1000\n",
      "1168/1168 [==============================] - 0s 137us/step - loss: 489736031.2329 - val_loss: 1224694698.0822\n",
      "Epoch 498/1000\n",
      "1168/1168 [==============================] - 0s 145us/step - loss: 489081637.8219 - val_loss: 1170958102.6849\n",
      "Epoch 499/1000\n",
      "1168/1168 [==============================] - 0s 137us/step - loss: 474304402.5616 - val_loss: 1299750165.6986\n",
      "Epoch 500/1000\n",
      "1168/1168 [==============================] - 0s 143us/step - loss: 480678782.3014 - val_loss: 1349386898.4110\n",
      "Epoch 501/1000\n",
      "1168/1168 [==============================] - 0s 139us/step - loss: 488951700.7945 - val_loss: 1210655627.1781\n",
      "Epoch 502/1000\n",
      "1168/1168 [==============================] - 0s 141us/step - loss: 467148991.8630 - val_loss: 1255161211.3973\n",
      "Epoch 503/1000\n",
      "1168/1168 [==============================] - 0s 154us/step - loss: 497256065.6986 - val_loss: 1252951906.3014\n",
      "Epoch 504/1000\n",
      "1168/1168 [==============================] - 0s 158us/step - loss: 481991543.3425 - val_loss: 1419264400.8219\n",
      "Epoch 505/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 510641576.6096 - val_loss: 1231623949.6986\n",
      "Epoch 506/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 467510086.1644 - val_loss: 1227157468.3836\n",
      "Epoch 507/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 463698199.3699 - val_loss: 1237300380.8219\n",
      "Epoch 508/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 484595147.0274 - val_loss: 1294132873.4247\n",
      "Epoch 509/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 467579643.7192 - val_loss: 1212612400.4384\n",
      "Epoch 510/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 477883418.3014 - val_loss: 1244513504.7671\n",
      "Epoch 511/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 494074332.4521 - val_loss: 1279072236.2740\n",
      "Epoch 512/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 457837570.3904 - val_loss: 1233528954.0822\n",
      "Epoch 513/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 486385969.3836 - val_loss: 1207433540.1644\n",
      "Epoch 514/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 458328990.0753 - val_loss: 1231559656.6575\n",
      "Epoch 515/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 495667957.7260 - val_loss: 1200431226.6301\n",
      "Epoch 516/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 448369417.2877 - val_loss: 1261801921.7534\n",
      "Epoch 517/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 561439407.1096 - val_loss: 1301278468.1644\n",
      "Epoch 518/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 461819830.1644 - val_loss: 1202552803.8356\n",
      "Epoch 519/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 120us/step - loss: 481283145.3288 - val_loss: 1412376208.4384\n",
      "Epoch 520/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 450374676.5137 - val_loss: 1188115725.4795\n",
      "Epoch 521/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 433027803.957 - 0s 121us/step - loss: 459054357.4041 - val_loss: 1262806858.7397\n",
      "Epoch 522/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 472384847.8425 - val_loss: 1235703743.6712\n",
      "Epoch 523/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 450800871.6986 - val_loss: 1216039947.9452\n",
      "Epoch 524/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 470975839.8219 - val_loss: 1189486957.9178\n",
      "Epoch 525/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 465712482.9521 - val_loss: 1224079687.9178\n",
      "Epoch 526/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 467269482.7808 - val_loss: 1250222631.5616\n",
      "Epoch 527/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 455592564.5274 - val_loss: 1220669865.2055\n",
      "Epoch 528/1000\n",
      "1168/1168 [==============================] - 0s 209us/step - loss: 468224760.6438 - val_loss: 1231130046.5753\n",
      "Epoch 529/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 445817591.3151 - val_loss: 1360229154.4110\n",
      "Epoch 530/1000\n",
      "1168/1168 [==============================] - 0s 143us/step - loss: 468028886.2123 - val_loss: 1230648790.5753\n",
      "Epoch 531/1000\n",
      "1168/1168 [==============================] - 0s 156us/step - loss: 436988335.2740 - val_loss: 1255194022.1370\n",
      "Epoch 532/1000\n",
      "1168/1168 [==============================] - 0s 150us/step - loss: 469766285.4452 - val_loss: 1224155891.0685\n",
      "Epoch 533/1000\n",
      "1168/1168 [==============================] - 0s 157us/step - loss: 461100984.3425 - val_loss: 1223215158.3562\n",
      "Epoch 534/1000\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 448296807.2055 - val_loss: 1313362622.9041\n",
      "Epoch 535/1000\n",
      "1168/1168 [==============================] - 0s 137us/step - loss: 472288978.6644 - val_loss: 1406222938.1918\n",
      "Epoch 536/1000\n",
      "1168/1168 [==============================] - 0s 132us/step - loss: 457472328.0137 - val_loss: 1319553681.0959\n",
      "Epoch 537/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 438776192.4247 - val_loss: 1204699034.6301\n",
      "Epoch 538/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 432970276.2192 - val_loss: 1264716031.2329\n",
      "Epoch 539/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 458589870.1781 - val_loss: 1268287650.8493\n",
      "Epoch 540/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 439432907.6096 - val_loss: 1261239222.1370\n",
      "Epoch 541/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 440790288.1507 - val_loss: 1363551579.6164\n",
      "Epoch 542/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 461383824.1644 - val_loss: 1422641186.2466\n",
      "Epoch 543/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 486918650.2740 - val_loss: 1196368105.0959\n",
      "Epoch 544/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 431004475.6301 - val_loss: 1235716193.9726\n",
      "Epoch 545/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 430348003.0479 - val_loss: 1240509794.4110\n",
      "Epoch 546/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 446236340.0205 - val_loss: 1224819954.3014\n",
      "Epoch 547/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 489428769.8219 - val_loss: 1308787185.3151\n",
      "Epoch 548/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 473964346.3014 - val_loss: 1221654899.3973\n",
      "Epoch 549/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 473380150.6438 - val_loss: 1277254420.9315\n",
      "Epoch 550/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 430043726.5000 - val_loss: 1211495934.1918\n",
      "Epoch 551/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 443211745.1781 - val_loss: 1293224626.0822\n",
      "Epoch 552/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 426103301.4932 - val_loss: 1231848684.9315\n",
      "Epoch 553/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 451727464.5411 - val_loss: 1223719792.3288\n",
      "Epoch 554/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 423248126.2945 - val_loss: 1361773275.7260\n",
      "Epoch 555/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 445307136.7671 - val_loss: 1276305330.8493\n",
      "Epoch 556/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 431220079.3973 - val_loss: 1291324482.3014\n",
      "Epoch 557/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 450540518.8219 - val_loss: 1239047119.8356\n",
      "Epoch 558/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 457689997.9144 - val_loss: 1271386124.2740\n",
      "Epoch 559/1000\n",
      "1168/1168 [==============================] - 0s 103us/step - loss: 426628199.9863 - val_loss: 1329604029.3151\n",
      "Epoch 560/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 428865700.5068 - val_loss: 1240413813.5890\n",
      "Epoch 561/1000\n",
      "1168/1168 [==============================] - 0s 141us/step - loss: 463780278.2192 - val_loss: 1237979124.1644\n",
      "Epoch 562/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 416910891.7671 - val_loss: 1247783375.5616\n",
      "Epoch 563/1000\n",
      "1168/1168 [==============================] - 0s 210us/step - loss: 435062061.3630 - val_loss: 1242737108.0548\n",
      "Epoch 564/1000\n",
      "1168/1168 [==============================] - 0s 157us/step - loss: 425222590.0616 - val_loss: 1276302130.7397\n",
      "Epoch 565/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 453451098.8219 - val_loss: 1261832528.0000\n",
      "Epoch 566/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 430599354.8630 - val_loss: 1312498639.0137\n",
      "Epoch 567/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 452079678.1507 - val_loss: 1348000218.9589\n",
      "Epoch 568/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 436730960.5068 - val_loss: 1367090574.1370\n",
      "Epoch 569/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 447451538.5822 - val_loss: 1335306412.0548\n",
      "Epoch 570/1000\n",
      "1168/1168 [==============================] - 0s 139us/step - loss: 423186878.0822 - val_loss: 1265262821.9178\n",
      "Epoch 571/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 431815046.1233 - val_loss: 1342897979.6164\n",
      "Epoch 572/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 442755181.7945 - val_loss: 1255660377.6986\n",
      "Epoch 573/1000\n",
      "1168/1168 [==============================] - 0s 136us/step - loss: 426299353.0137 - val_loss: 1249701508.8219\n",
      "Epoch 574/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 432732665.0000 - val_loss: 1524348545.0411\n",
      "Epoch 575/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 422383004.0685 - val_loss: 1276032244.7123\n",
      "Epoch 576/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 414604649.2055 - val_loss: 1451278054.2466\n",
      "Epoch 577/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 450380667.5479 - val_loss: 1242163631.2329\n",
      "Epoch 578/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 424669665.4384 - val_loss: 1290591129.9726\n",
      "Epoch 579/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 431732043.5479 - val_loss: 1278894118.4658\n",
      "Epoch 580/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 407925415.8904 - val_loss: 1322139568.4384\n",
      "Epoch 581/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 408405705.4178 - val_loss: 1333448122.4658\n",
      "Epoch 582/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 421953829.1027 - val_loss: 1443256228.9315\n",
      "Epoch 583/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 467126404.3836 - val_loss: 1328482736.4384\n",
      "Epoch 584/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 115us/step - loss: 395576304.0068 - val_loss: 1316536425.7534\n",
      "Epoch 585/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 403533015.9863 - val_loss: 1367482896.2192\n",
      "Epoch 586/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 398059626.6918 - val_loss: 1405114488.7671\n",
      "Epoch 587/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 428530406.7055 - val_loss: 1375089715.8356\n",
      "Epoch 588/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 418616163.7295 - val_loss: 1322275651.6164\n",
      "Epoch 589/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 403378969.4178 - val_loss: 1275491970.7397\n",
      "Epoch 590/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 402877397.6644 - val_loss: 1273861624.5479\n",
      "Epoch 591/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 410753844.1027 - val_loss: 1286002428.6027\n",
      "Epoch 592/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 467033502.8151 - val_loss: 1298258330.7397\n",
      "Epoch 593/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 449924542.9863 - val_loss: 1284598439.4521\n",
      "Epoch 594/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 447155737.4658 - val_loss: 1206406647.1233\n",
      "Epoch 595/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 426801611.6301 - val_loss: 1385861229.1507\n",
      "Epoch 596/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 415173007.0411 - val_loss: 1370617035.0685\n",
      "Epoch 597/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 428516759.8356 - val_loss: 1319741784.4384\n",
      "Epoch 598/1000\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 422647998.3699 - val_loss: 1349301574.2466\n",
      "Epoch 599/1000\n",
      "1168/1168 [==============================] - 0s 202us/step - loss: 416716203.2466 - val_loss: 1315239913.0959\n",
      "Epoch 600/1000\n",
      "1168/1168 [==============================] - 0s 156us/step - loss: 418313932.3973 - val_loss: 1513031013.4795\n",
      "Epoch 601/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 406991500.6233 - val_loss: 1316390401.2055\n",
      "Epoch 602/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 398998841.7945 - val_loss: 1300422101.8082\n",
      "Epoch 603/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 391444955.8836 - val_loss: 1365480960.0000\n",
      "Epoch 604/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 404729944.4658 - val_loss: 1301468123.0685\n",
      "Epoch 605/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 411731903.0411 - val_loss: 1375470970.1918\n",
      "Epoch 606/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 400753240.1918 - val_loss: 1289107400.3288\n",
      "Epoch 607/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 387291305.1575 - val_loss: 1287807355.3973\n",
      "Epoch 608/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 443784189.3493 - val_loss: 1348944163.2877\n",
      "Epoch 609/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 407021076.5719 - val_loss: 1311703220.2740\n",
      "Epoch 610/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 408156955.5000 - val_loss: 1326806674.8493\n",
      "Epoch 611/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 403563176.7192 - val_loss: 1304878648.2192\n",
      "Epoch 612/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 397323653.7774 - val_loss: 1333803371.8356\n",
      "Epoch 613/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 407042061.3151 - val_loss: 1274563169.5342\n",
      "Epoch 614/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 411786470.5890 - val_loss: 1349789341.5890\n",
      "Epoch 615/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 381836430.1644 - val_loss: 1355970259.1781\n",
      "Epoch 616/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 386567105.1096 - val_loss: 1460840095.2329\n",
      "Epoch 617/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 397827789.4452 - val_loss: 1322865307.2877\n",
      "Epoch 618/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 393451266.2671 - val_loss: 1362168825.2603\n",
      "Epoch 619/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 394714199.6301 - val_loss: 1397122212.9315\n",
      "Epoch 620/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 405130485.7397 - val_loss: 1362431868.3836\n",
      "Epoch 621/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 383450857.2774 - val_loss: 1346675277.8082\n",
      "Epoch 622/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 380815437.1062 - val_loss: 1337570686.1370\n",
      "Epoch 623/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 395452090.0137 - val_loss: 1365085505.8630\n",
      "Epoch 624/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 427197612.8493 - val_loss: 1306233570.5205\n",
      "Epoch 625/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 375289832.5651 - val_loss: 1327630140.0000\n",
      "Epoch 626/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 426012451.2329 - val_loss: 1774480943.9041\n",
      "Epoch 627/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 398821640.7329 - val_loss: 1466161503.1233\n",
      "Epoch 628/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 410846684.0000 - val_loss: 1342643515.5068\n",
      "Epoch 629/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 382678393.2877 - val_loss: 1358931763.8356\n",
      "Epoch 630/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 389452745.0753 - val_loss: 1415481924.3836\n",
      "Epoch 631/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 381275925.7055 - val_loss: 1386713289.2055\n",
      "Epoch 632/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 396620661.0000 - val_loss: 1474053557.5890\n",
      "Epoch 633/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 407805734.6986 - val_loss: 1358118317.7534\n",
      "Epoch 634/1000\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 378046451.1986 - val_loss: 1331849621.9178\n",
      "Epoch 635/1000\n",
      "1168/1168 [==============================] - 0s 210us/step - loss: 401043215.4932 - val_loss: 1401129206.1370\n",
      "Epoch 636/1000\n",
      "1168/1168 [==============================] - 0s 219us/step - loss: 379737723.0342 - val_loss: 1431327398.5753\n",
      "Epoch 637/1000\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 374945269.7671 - val_loss: 1356780368.2740\n",
      "Epoch 638/1000\n",
      "1168/1168 [==============================] - 0s 155us/step - loss: 396751286.5548 - val_loss: 1363125205.8082\n",
      "Epoch 639/1000\n",
      "1168/1168 [==============================] - 0s 156us/step - loss: 398752977.0342 - val_loss: 1387799337.5342\n",
      "Epoch 640/1000\n",
      "1168/1168 [==============================] - 0s 150us/step - loss: 411683484.8493 - val_loss: 1375546575.1233\n",
      "Epoch 641/1000\n",
      "1168/1168 [==============================] - 0s 152us/step - loss: 392439806.0274 - val_loss: 1353550385.5342\n",
      "Epoch 642/1000\n",
      "1168/1168 [==============================] - 0s 140us/step - loss: 435513452.8151 - val_loss: 1419192456.3288\n",
      "Epoch 643/1000\n",
      "1168/1168 [==============================] - 0s 137us/step - loss: 400229666.2603 - val_loss: 1427566124.7123\n",
      "Epoch 644/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 390180314.8082 - val_loss: 1357563535.6712\n",
      "Epoch 645/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 419069424.2740 - val_loss: 1378417369.8630\n",
      "Epoch 646/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 390142618.5959 - val_loss: 1314514306.3014\n",
      "Epoch 647/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 399460942.5959 - val_loss: 1337806845.1507\n",
      "Epoch 648/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 394279433.8973 - val_loss: 1406158287.1233\n",
      "Epoch 649/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 142us/step - loss: 439698242.8014 - val_loss: 1321600751.8904\n",
      "Epoch 650/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 379990391.4658 - val_loss: 1358553925.9178\n",
      "Epoch 651/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 384069544.0616 - val_loss: 1389423782.4658\n",
      "Epoch 652/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 394463994.2260 - val_loss: 1422620746.7397\n",
      "Epoch 653/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 391017570.2808 - val_loss: 1341563332.2740\n",
      "Epoch 654/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 399573090.8014 - val_loss: 1335033655.2329\n",
      "Epoch 655/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 375643857.5959 - val_loss: 1374486804.0548\n",
      "Epoch 656/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 371006937.2260 - val_loss: 1350940763.8356\n",
      "Epoch 657/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 380357821.4247 - val_loss: 1552754865.3699\n",
      "Epoch 658/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 414775784.9452 - val_loss: 1442591131.1781\n",
      "Epoch 659/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 385349526.8904 - val_loss: 1395366417.4247\n",
      "Epoch 660/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 364062744.2603 - val_loss: 1351840673.1507\n",
      "Epoch 661/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 387803685.5616 - val_loss: 1394180230.4658\n",
      "Epoch 662/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 400075046.9315 - val_loss: 1471387242.4110\n",
      "Epoch 663/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 366576066.8288 - val_loss: 1468157360.1096\n",
      "Epoch 664/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 401451252.9178 - val_loss: 1495965088.7123\n",
      "Epoch 665/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 389517068.8219 - val_loss: 1380522064.3288\n",
      "Epoch 666/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 376927957.0959 - val_loss: 1334447104.1644\n",
      "Epoch 667/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 381367160.6986 - val_loss: 1497441915.9452\n",
      "Epoch 668/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 378802443.8082 - val_loss: 1479673554.6849\n",
      "Epoch 669/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 369499436.2055 - val_loss: 1465198073.6438\n",
      "Epoch 670/1000\n",
      "1168/1168 [==============================] - 0s 207us/step - loss: 350766588.8904 - val_loss: 1493631118.1370\n",
      "Epoch 671/1000\n",
      "1168/1168 [==============================] - 0s 198us/step - loss: 364585885.7397 - val_loss: 1393642000.3288\n",
      "Epoch 672/1000\n",
      "1168/1168 [==============================] - 0s 154us/step - loss: 363844895.3014 - val_loss: 1390630392.7123\n",
      "Epoch 673/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 406936505.4932 - val_loss: 1425700694.7945\n",
      "Epoch 674/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 377989582.8116 - val_loss: 1512960344.6575\n",
      "Epoch 675/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 393211968.7911 - val_loss: 1363972885.7534\n",
      "Epoch 676/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 366687835.2740 - val_loss: 1468863629.3699\n",
      "Epoch 677/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 383354019.6096 - val_loss: 1538544622.6301\n",
      "Epoch 678/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 366336253.8493 - val_loss: 1526337089.4247\n",
      "Epoch 679/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 391876482.4658 - val_loss: 1375861014.5753\n",
      "Epoch 680/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 422704495.4932 - val_loss: 1434412479.4521\n",
      "Epoch 681/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 382173707.6301 - val_loss: 1373788883.8356\n",
      "Epoch 682/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 385126579.0822 - val_loss: 1390086665.3151\n",
      "Epoch 683/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 362802793.9178 - val_loss: 1439283698.3014\n",
      "Epoch 684/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 373770129.6096 - val_loss: 1361336187.5068\n",
      "Epoch 685/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 367514061.2123 - val_loss: 1358542853.4795\n",
      "Epoch 686/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 385349077.3219 - val_loss: 1395747136.7671\n",
      "Epoch 687/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 367018567.5411 - val_loss: 1556272900.4932\n",
      "Epoch 688/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 368389561.4726 - val_loss: 1442624127.2329\n",
      "Epoch 689/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 368063859.5788 - val_loss: 1375761324.9315\n",
      "Epoch 690/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 363439490.7397 - val_loss: 1416488944.1096\n",
      "Epoch 691/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 376563662.9041 - val_loss: 1418961534.7945\n",
      "Epoch 692/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 363605840.2877 - val_loss: 1423136453.9178\n",
      "Epoch 693/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 385017217.5753 - val_loss: 1380395520.3288\n",
      "Epoch 694/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 396766984.9452 - val_loss: 1598071216.6575\n",
      "Epoch 695/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 418106293.6438 - val_loss: 1427152148.7123\n",
      "Epoch 696/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 365092773.1918 - val_loss: 1422570853.3699\n",
      "Epoch 697/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 375682724.4452 - val_loss: 1488843309.2603\n",
      "Epoch 698/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 375378095.1438 - val_loss: 1438511050.1918\n",
      "Epoch 699/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 354475411.9384 - val_loss: 1386561277.5890\n",
      "Epoch 700/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 378790442.7945 - val_loss: 1375486718.7397\n",
      "Epoch 701/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 357823584.5342 - val_loss: 1384189247.8904\n",
      "Epoch 702/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 363655602.4384 - val_loss: 1424039573.6986\n",
      "Epoch 703/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 376300042.0342 - val_loss: 1397099316.2740\n",
      "Epoch 704/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 363917012.5582 - val_loss: 1366733027.9452\n",
      "Epoch 705/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 376250990.2705 - val_loss: 1385878660.4932\n",
      "Epoch 706/1000\n",
      "1168/1168 [==============================] - 0s 187us/step - loss: 355218565.5068 - val_loss: 1367689081.9726\n",
      "Epoch 707/1000\n",
      "1168/1168 [==============================] - 0s 235us/step - loss: 351482216.8082 - val_loss: 1387268181.6986\n",
      "Epoch 708/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 370518254.9521 - val_loss: 1368814927.8904\n",
      "Epoch 709/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 389818836.3014 - val_loss: 1467336242.9589\n",
      "Epoch 710/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 371291597.6027 - val_loss: 1402127624.4384\n",
      "Epoch 711/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 353363986.9863 - val_loss: 1429501088.3288\n",
      "Epoch 712/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 415750605.2671 - val_loss: 1320772940.8219\n",
      "Epoch 713/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 361503247.8082 - val_loss: 1446285684.7123\n",
      "Epoch 714/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 122us/step - loss: 345134164.2397 - val_loss: 1527182291.5068\n",
      "Epoch 715/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 370334811.6781 - val_loss: 1396731731.3425\n",
      "Epoch 716/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 358720388.3836 - val_loss: 1465504048.9863\n",
      "Epoch 717/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 350829860.7123 - val_loss: 1464409942.6849\n",
      "Epoch 718/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 355668162.7671 - val_loss: 1409535792.8767\n",
      "Epoch 719/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 370332758.5479 - val_loss: 1366352251.8904\n",
      "Epoch 720/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 365692633.1164 - val_loss: 1427427290.4110\n",
      "Epoch 721/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 350330871.5479 - val_loss: 1465375283.0685\n",
      "Epoch 722/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 374524105.1233 - val_loss: 1421220558.0822\n",
      "Epoch 723/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 373210947.7397 - val_loss: 1475836934.3562\n",
      "Epoch 724/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 385694563.6507 - val_loss: 1314046323.5068\n",
      "Epoch 725/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 367233511.7945 - val_loss: 1564154253.4795\n",
      "Epoch 726/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 397270548.0479 - val_loss: 1459103698.8493\n",
      "Epoch 727/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 372062570.5342 - val_loss: 1416030360.8767\n",
      "Epoch 728/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 370736766.2466 - val_loss: 1383833457.9726\n",
      "Epoch 729/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 351362066.1507 - val_loss: 1403110721.2055\n",
      "Epoch 730/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 357663231.1986 - val_loss: 1417199455.7808\n",
      "Epoch 731/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 348081097.5548 - val_loss: 1433897963.7260\n",
      "Epoch 732/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 356132894.9863 - val_loss: 1394772249.4795\n",
      "Epoch 733/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 369460628.2329 - val_loss: 1455819469.6986\n",
      "Epoch 734/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 365137373.3562 - val_loss: 1561666261.1507\n",
      "Epoch 735/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 366106118.1027 - val_loss: 1493072316.8219\n",
      "Epoch 736/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 375836451.5753 - val_loss: 1487867572.3836\n",
      "Epoch 737/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 346169439.4041 - val_loss: 1582205775.5616\n",
      "Epoch 738/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 366436839.9658 - val_loss: 1360246069.0411\n",
      "Epoch 739/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 343490837.1233 - val_loss: 1475615769.9726\n",
      "Epoch 740/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 393733639.5616 - val_loss: 1497135953.0959\n",
      "Epoch 741/1000\n",
      "1168/1168 [==============================] - 0s 136us/step - loss: 336347085.9315 - val_loss: 1432451221.2603\n",
      "Epoch 742/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 376155817.9315 - val_loss: 1509723480.1096\n",
      "Epoch 743/1000\n",
      "1168/1168 [==============================] - 0s 212us/step - loss: 353329976.2260 - val_loss: 1328329128.0000\n",
      "Epoch 744/1000\n",
      "1168/1168 [==============================] - 0s 225us/step - loss: 347751988.2603 - val_loss: 1579485456.7534\n",
      "Epoch 745/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 340795897.9555 - val_loss: 1489717633.7534\n",
      "Epoch 746/1000\n",
      "1168/1168 [==============================] - 0s 145us/step - loss: 372186377.9110 - val_loss: 1462102137.0959\n",
      "Epoch 747/1000\n",
      "1168/1168 [==============================] - 0s 136us/step - loss: 357591781.7808 - val_loss: 1698733074.9589\n",
      "Epoch 748/1000\n",
      "1168/1168 [==============================] - 0s 143us/step - loss: 367167063.6986 - val_loss: 1409189673.8630\n",
      "Epoch 749/1000\n",
      "1168/1168 [==============================] - 0s 139us/step - loss: 349817843.6301 - val_loss: 1435936717.1507\n",
      "Epoch 750/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 354392238.3699 - val_loss: 1377467821.8082\n",
      "Epoch 751/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 366876825.7671 - val_loss: 1464915353.5890\n",
      "Epoch 752/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 377717755.0616 - val_loss: 1377244672.5479\n",
      "Epoch 753/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 362865947.3425 - val_loss: 1490225033.0959\n",
      "Epoch 754/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 368666347.8082 - val_loss: 1406906728.1096\n",
      "Epoch 755/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 348404368.5342 - val_loss: 1626737289.6986\n",
      "Epoch 756/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 333482318.2055 - val_loss: 1493601804.3836\n",
      "Epoch 757/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 345351214.3219 - val_loss: 1452606672.0000\n",
      "Epoch 758/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 377992805.8151 - val_loss: 1518357253.8630\n",
      "Epoch 759/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 331000068.3288 - val_loss: 1445393291.0685\n",
      "Epoch 760/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 338776799.3288 - val_loss: 1558030958.2466\n",
      "Epoch 761/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 356905936.0274 - val_loss: 1454281063.3425\n",
      "Epoch 762/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 343245512.1712 - val_loss: 1401515467.0685\n",
      "Epoch 763/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 361920913.3425 - val_loss: 1533234163.3973\n",
      "Epoch 764/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 331570061.7534 - val_loss: 1383801747.7260\n",
      "Epoch 765/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 362015262.3904 - val_loss: 1476262856.7671\n",
      "Epoch 766/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 340683254.1370 - val_loss: 1501438627.8904\n",
      "Epoch 767/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 360684327.2192 - val_loss: 1652399362.6575\n",
      "Epoch 768/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 375976426.4589 - val_loss: 1495431758.2466\n",
      "Epoch 769/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 353213295.3356 - val_loss: 1506832648.4384\n",
      "Epoch 770/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 361418833.3973 - val_loss: 1545722944.0000\n",
      "Epoch 771/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 368798837.9589 - val_loss: 1524242704.5479\n",
      "Epoch 772/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 347255751.8767 - val_loss: 1505890409.4247\n",
      "Epoch 773/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 339346995.5890 - val_loss: 1423898803.0137\n",
      "Epoch 774/1000\n",
      "1168/1168 [==============================] - 0s 149us/step - loss: 343114801.2740 - val_loss: 1583790212.3288\n",
      "Epoch 775/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 333155871.6986 - val_loss: 1425202159.1233\n",
      "Epoch 776/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 376299937.7671 - val_loss: 1570612791.8356\n",
      "Epoch 777/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 354138460.2123 - val_loss: 1526980704.1096\n",
      "Epoch 778/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 375955663.3699 - val_loss: 1428997643.1781\n",
      "Epoch 779/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 208us/step - loss: 336023031.7671 - val_loss: 1528214305.0959\n",
      "Epoch 780/1000\n",
      "1168/1168 [==============================] - 0s 163us/step - loss: 328363775.0822 - val_loss: 1502206676.1644\n",
      "Epoch 781/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 361188987.4452 - val_loss: 1458224239.5616\n",
      "Epoch 782/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 337556803.7534 - val_loss: 1407494133.4247\n",
      "Epoch 783/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 353193230.2877 - val_loss: 1505186220.0548\n",
      "Epoch 784/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 333575453.8425 - val_loss: 1498934852.7123\n",
      "Epoch 785/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 323489929.5616 - val_loss: 1465559543.8904\n",
      "Epoch 786/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 334833093.3699 - val_loss: 1523567446.3014\n",
      "Epoch 787/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 343254385.3151 - val_loss: 1482174499.5068\n",
      "Epoch 788/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 344995532.6507 - val_loss: 1515588428.1644\n",
      "Epoch 789/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 376143247.5890 - val_loss: 1445330800.8767\n",
      "Epoch 790/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 326152187.9178 - val_loss: 1467625656.4384\n",
      "Epoch 791/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 336964062.0068 - val_loss: 1498251372.6027\n",
      "Epoch 792/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 363461773.6712 - val_loss: 1442501071.6712\n",
      "Epoch 793/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 349064139.1712 - val_loss: 1544910421.2603\n",
      "Epoch 794/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 342036555.2123 - val_loss: 1434247083.4521\n",
      "Epoch 795/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 349533628.0411 - val_loss: 1618530094.2740\n",
      "Epoch 796/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 361264686.4452 - val_loss: 1461256889.2055\n",
      "Epoch 797/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 343022347.5685 - val_loss: 1502884406.1918\n",
      "Epoch 798/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 343953473.9795 - val_loss: 1454781822.6849\n",
      "Epoch 799/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 325477999.6918 - val_loss: 1481280565.0411\n",
      "Epoch 800/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 327971958.9795 - val_loss: 1976999238.4384\n",
      "Epoch 801/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 341339557.5959 - val_loss: 1561798725.2603\n",
      "Epoch 802/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 342561890.8630 - val_loss: 1617698513.5342\n",
      "Epoch 803/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 359390393.8425 - val_loss: 1536828917.1507\n",
      "Epoch 804/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 354859188.6781 - val_loss: 1490415839.2877\n",
      "Epoch 805/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 361537984.7603 - val_loss: 1515588660.4932\n",
      "Epoch 806/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 334274286.0411 - val_loss: 1586911134.8493\n",
      "Epoch 807/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 339494366.1575 - val_loss: 1806020110.8904\n",
      "Epoch 808/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 366940836.3151 - val_loss: 1533715608.2192\n",
      "Epoch 809/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 337155595.8219 - val_loss: 1505463081.5342\n",
      "Epoch 810/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 340529242.7260 - val_loss: 1462070054.9041\n",
      "Epoch 811/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 341239249.2192 - val_loss: 1516737041.7534\n",
      "Epoch 812/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 349352760.9041 - val_loss: 1434437440.3288\n",
      "Epoch 813/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 335675472.0137 - val_loss: 1511987458.4110\n",
      "Epoch 814/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 345993075.3425 - val_loss: 1432383733.8082\n",
      "Epoch 815/1000\n",
      "1168/1168 [==============================] - 0s 183us/step - loss: 339311867.9384 - val_loss: 1459008893.4247\n",
      "Epoch 816/1000\n",
      "1168/1168 [==============================] - 0s 212us/step - loss: 361946195.5959 - val_loss: 1551805749.0411\n",
      "Epoch 817/1000\n",
      "1168/1168 [==============================] - 0s 153us/step - loss: 331270649.2534 - val_loss: 1499437955.2877\n",
      "Epoch 818/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 320032782.6918 - val_loss: 1475363898.1918\n",
      "Epoch 819/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 318885212.1027 - val_loss: 1619251687.8904\n",
      "Epoch 820/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 376885930.6027 - val_loss: 1510379170.4110\n",
      "Epoch 821/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 356995001.8767 - val_loss: 1543763585.3151\n",
      "Epoch 822/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 327036846.4452 - val_loss: 1530115517.5890\n",
      "Epoch 823/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 337948477.3562 - val_loss: 1474374747.2329\n",
      "Epoch 824/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 343024555.1986 - val_loss: 1627248150.8493\n",
      "Epoch 825/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 338036395.1507 - val_loss: 1527811419.9452\n",
      "Epoch 826/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 322458875.4041 - val_loss: 1512614799.6712\n",
      "Epoch 827/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 347351794.6370 - val_loss: 1458136282.1370\n",
      "Epoch 828/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 327294502.1849 - val_loss: 1512973646.5753\n",
      "Epoch 829/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 332248607.0411 - val_loss: 1547397111.1233\n",
      "Epoch 830/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 348729163.8288 - val_loss: 1437483618.1918\n",
      "Epoch 831/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 323486656.5753 - val_loss: 1740762103.7260\n",
      "Epoch 832/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 349744735.1781 - val_loss: 1529411036.4384\n",
      "Epoch 833/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 338866096.4658 - val_loss: 1509134152.7671\n",
      "Epoch 834/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 310801446.5068 - val_loss: 1544959530.6301\n",
      "Epoch 835/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 328653793.3288 - val_loss: 1550808313.3151\n",
      "Epoch 836/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 343662631.6370 - val_loss: 1548819337.2603\n",
      "Epoch 837/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 319965269.1438 - val_loss: 1426590266.1918\n",
      "Epoch 838/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 350209574.2466 - val_loss: 1652137664.4110\n",
      "Epoch 839/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 342595787.6027 - val_loss: 1479232239.0137\n",
      "Epoch 840/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 322350574.3904 - val_loss: 1434297756.2192\n",
      "Epoch 841/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 336360869.2877 - val_loss: 1606582576.8767\n",
      "Epoch 842/1000\n",
      "1168/1168 [==============================] - 0s 136us/step - loss: 334385254.1438 - val_loss: 1454289536.9863\n",
      "Epoch 843/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 309829778.5685 - val_loss: 1530743104.1096\n",
      "Epoch 844/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 115us/step - loss: 347779679.0822 - val_loss: 1457934956.1644\n",
      "Epoch 845/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 328023623.2740 - val_loss: 1526407256.3288\n",
      "Epoch 846/1000\n",
      "1168/1168 [==============================] - 0s 138us/step - loss: 329185519.3630 - val_loss: 1497271703.5616\n",
      "Epoch 847/1000\n",
      "1168/1168 [==============================] - 0s 142us/step - loss: 328337783.8459 - val_loss: 1434277026.9589\n",
      "Epoch 848/1000\n",
      "1168/1168 [==============================] - 0s 140us/step - loss: 305374136.7123 - val_loss: 1513765728.0000\n",
      "Epoch 849/1000\n",
      "1168/1168 [==============================] - 0s 157us/step - loss: 330035244.0753 - val_loss: 1597904467.2877\n",
      "Epoch 850/1000\n",
      "1168/1168 [==============================] - 0s 187us/step - loss: 363640970.8014 - val_loss: 1531599842.7397\n",
      "Epoch 851/1000\n",
      "1168/1168 [==============================] - 0s 225us/step - loss: 326159810.3151 - val_loss: 1665468738.6301\n",
      "Epoch 852/1000\n",
      "1168/1168 [==============================] - 0s 194us/step - loss: 324157591.9589 - val_loss: 1467591497.6986\n",
      "Epoch 853/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 329991722.4795 - val_loss: 1627759923.0137\n",
      "Epoch 854/1000\n",
      "1168/1168 [==============================] - 0s 136us/step - loss: 339988707.2123 - val_loss: 1663968478.2466\n",
      "Epoch 855/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 335128357.0068 - val_loss: 1564645125.5890\n",
      "Epoch 856/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 335055108.7568 - val_loss: 1444969453.0411\n",
      "Epoch 857/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 354153778.3082 - val_loss: 1497360251.1781\n",
      "Epoch 858/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 326287221.9863 - val_loss: 1477637467.2877\n",
      "Epoch 859/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 312354421.9384 - val_loss: 1617391324.9863\n",
      "Epoch 860/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 347960621.8082 - val_loss: 1488959520.8767\n",
      "Epoch 861/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 335389125.8973 - val_loss: 1442674595.3973\n",
      "Epoch 862/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 370416952.2192 - val_loss: 1504346419.5068\n",
      "Epoch 863/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 329290910.5000 - val_loss: 1532313824.3288\n",
      "Epoch 864/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 311150207.4110 - val_loss: 1536495284.6575\n",
      "Epoch 865/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 326882177.7534 - val_loss: 1683060658.0822\n",
      "Epoch 866/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 338272645.5822 - val_loss: 1594454571.3973\n",
      "Epoch 867/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 320212354.1507 - val_loss: 1513782467.5068\n",
      "Epoch 868/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 327464215.9452 - val_loss: 1619320014.0274\n",
      "Epoch 869/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 343769003.3904 - val_loss: 1557692725.5342\n",
      "Epoch 870/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 339270587.9863 - val_loss: 1518513119.4521\n",
      "Epoch 871/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 335331631.5822 - val_loss: 1493396968.1096\n",
      "Epoch 872/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 312691163.0342 - val_loss: 1522742860.8219\n",
      "Epoch 873/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 317748288.4110 - val_loss: 1505080001.1507\n",
      "Epoch 874/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 330062593.1438 - val_loss: 1522189582.6849\n",
      "Epoch 875/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 309638734.9726 - val_loss: 1451760289.6438\n",
      "Epoch 876/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 318654105.7740 - val_loss: 1635178132.3288\n",
      "Epoch 877/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 326231450.2123 - val_loss: 1557204914.0822\n",
      "Epoch 878/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 317443555.2363 - val_loss: 1639317934.3562\n",
      "Epoch 879/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 339535687.6233 - val_loss: 1427679222.2466\n",
      "Epoch 880/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 318966107.2192 - val_loss: 1473702000.7671\n",
      "Epoch 881/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 305673363.2260 - val_loss: 1572887665.7534\n",
      "Epoch 882/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 322212937.5822 - val_loss: 1499987727.2329\n",
      "Epoch 883/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 331874792.0000 - val_loss: 1472086766.2466\n",
      "Epoch 884/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 310609035.1164 - val_loss: 1506403736.5479\n",
      "Epoch 885/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 315065006.8425 - val_loss: 1590979907.1781\n",
      "Epoch 886/1000\n",
      "1168/1168 [==============================] - 0s 160us/step - loss: 311352545.3356 - val_loss: 1648361293.0411\n",
      "Epoch 887/1000\n",
      "1168/1168 [==============================] - 0s 216us/step - loss: 311484335.0274 - val_loss: 1538914965.1507\n",
      "Epoch 888/1000\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 311941929.0890 - val_loss: 1476770771.0137\n",
      "Epoch 889/1000\n",
      "1168/1168 [==============================] - 0s 142us/step - loss: 336125557.2877 - val_loss: 1538263044.6027\n",
      "Epoch 890/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 321185775.3356 - val_loss: 1537433845.2603\n",
      "Epoch 891/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 314859377.8562 - val_loss: 1849010244.0274\n",
      "Epoch 892/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 339491629.3014 - val_loss: 1476697704.9863\n",
      "Epoch 893/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 306759988.6164 - val_loss: 1551764851.7260\n",
      "Epoch 894/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 339138264.1096 - val_loss: 1691747936.6575\n",
      "Epoch 895/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 343804087.6507 - val_loss: 1637993883.7260\n",
      "Epoch 896/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 316764060.3836 - val_loss: 1502954164.0000\n",
      "Epoch 897/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 337228177.5753 - val_loss: 1585845110.6849\n",
      "Epoch 898/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 328037482.1918 - val_loss: 1506801177.4247\n",
      "Epoch 899/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 300243548.5137 - val_loss: 1497300625.5342\n",
      "Epoch 900/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 352412888.5753 - val_loss: 1568917338.6301\n",
      "Epoch 901/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 303378731.0068 - val_loss: 1518171341.5890\n",
      "Epoch 902/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 314046011.5753 - val_loss: 1491160036.4932\n",
      "Epoch 903/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 301495838.1062 - val_loss: 1546119560.9863\n",
      "Epoch 904/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 323328103.5205 - val_loss: 1575692838.2466\n",
      "Epoch 905/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 316629824.7603 - val_loss: 1624908160.4384\n",
      "Epoch 906/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 311182567.2038 - val_loss: 1639110343.2329\n",
      "Epoch 907/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 335742144.1233 - val_loss: 1502350776.4384\n",
      "Epoch 908/1000\n",
      "1168/1168 [==============================] - 0s 137us/step - loss: 295048160.6781 - val_loss: 1492629581.1507\n",
      "Epoch 909/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 122us/step - loss: 303833218.7192 - val_loss: 1454090072.3836\n",
      "Epoch 910/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 313896769.9795 - val_loss: 1465967542.7397\n",
      "Epoch 911/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 317206141.3973 - val_loss: 1594308636.1096\n",
      "Epoch 912/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 304719030.0342 - val_loss: 1467257298.4110\n",
      "Epoch 913/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 316341861.5890 - val_loss: 1437739049.0959\n",
      "Epoch 914/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 337218983.5685 - val_loss: 1446933211.0685\n",
      "Epoch 915/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 305148834.9041 - val_loss: 1514959654.5205\n",
      "Epoch 916/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 289624238.5000 - val_loss: 1535340048.6027\n",
      "Epoch 917/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 327240964.9932 - val_loss: 1514890633.7534\n",
      "Epoch 918/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 341796421.3493 - val_loss: 1567208299.7260\n",
      "Epoch 919/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 315188371.8288 - val_loss: 1478144644.8219\n",
      "Epoch 920/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 317409311.7740 - val_loss: 1550275903.7808\n",
      "Epoch 921/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 344300209.1986 - val_loss: 1673688413.3151\n",
      "Epoch 922/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 286557604.2295 - val_loss: 1467040473.4247\n",
      "Epoch 923/1000\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 314569736.4760 - val_loss: 1529500181.8082\n",
      "Epoch 924/1000\n",
      "1168/1168 [==============================] - 0s 214us/step - loss: 320394074.8836 - val_loss: 1441069779.9452\n",
      "Epoch 925/1000\n",
      "1168/1168 [==============================] - 0s 151us/step - loss: 305005300.9658 - val_loss: 1561169159.2329\n",
      "Epoch 926/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 305518965.6849 - val_loss: 1608838685.5342\n",
      "Epoch 927/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 305918347.7945 - val_loss: 1536569411.6164\n",
      "Epoch 928/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 309393135.0411 - val_loss: 1575159079.2329\n",
      "Epoch 929/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 336793495.2877 - val_loss: 1578007407.5616\n",
      "Epoch 930/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 333330252.6918 - val_loss: 1510380915.8356\n",
      "Epoch 931/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 279013743.4521 - val_loss: 1648417225.4247\n",
      "Epoch 932/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 322645466.6712 - val_loss: 1474526306.2466\n",
      "Epoch 933/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 292764048.3767 - val_loss: 1479949676.1644\n",
      "Epoch 934/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 304154434.9658 - val_loss: 1564001883.0137\n",
      "Epoch 935/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 339705510.2603 - val_loss: 1498093479.0137\n",
      "Epoch 936/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 296513214.2329 - val_loss: 1464934547.6164\n",
      "Epoch 937/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 297605043.6438 - val_loss: 1468164997.2603\n",
      "Epoch 938/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 336612020.0411 - val_loss: 1657187097.8082\n",
      "Epoch 939/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 306414195.7260 - val_loss: 1628916693.1233\n",
      "Epoch 940/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 316203726.3630 - val_loss: 1516732839.5616\n",
      "Epoch 941/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 293744516.2123 - val_loss: 1493561165.3699\n",
      "Epoch 942/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 303928759.6712 - val_loss: 1612685468.8767\n",
      "Epoch 943/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 322715237.3151 - val_loss: 1582191228.4384\n",
      "Epoch 944/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 300030371.7466 - val_loss: 1523317594.6849\n",
      "Epoch 945/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 325572057.7123 - val_loss: 1531781352.6575\n",
      "Epoch 946/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 302468809.5137 - val_loss: 1404206064.6575\n",
      "Epoch 947/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 345383257.6233 - val_loss: 1547344690.6301\n",
      "Epoch 948/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 308357473.8767 - val_loss: 1502501771.5068\n",
      "Epoch 949/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 289995912.0651 - val_loss: 1497770289.8630\n",
      "Epoch 950/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 284036867.0548 - val_loss: 1497996962.7945\n",
      "Epoch 951/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 289982438.5685 - val_loss: 1533586341.5342\n",
      "Epoch 952/1000\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 325252601.5616 - val_loss: 1517512620.1644\n",
      "Epoch 953/1000\n",
      "1168/1168 [==============================] - 0s 148us/step - loss: 347821020.0616 - val_loss: 1549860869.9178\n",
      "Epoch 954/1000\n",
      "1168/1168 [==============================] - 0s 145us/step - loss: 315149911.5000 - val_loss: 1467783222.0274\n",
      "Epoch 955/1000\n",
      "1168/1168 [==============================] - 0s 153us/step - loss: 292126786.6473 - val_loss: 1587803528.2192\n",
      "Epoch 956/1000\n",
      "1168/1168 [==============================] - 0s 153us/step - loss: 333555140.1507 - val_loss: 1853588325.3219\n",
      "Epoch 957/1000\n",
      "1168/1168 [==============================] - 0s 148us/step - loss: 330832658.3151 - val_loss: 1520154376.1096\n",
      "Epoch 958/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 301445013.5616 - val_loss: 1494122841.7534\n",
      "Epoch 959/1000\n",
      "1168/1168 [==============================] - 0s 213us/step - loss: 283457575.8288 - val_loss: 1645081091.7260\n",
      "Epoch 960/1000\n",
      "1168/1168 [==============================] - 0s 210us/step - loss: 311783115.5582 - val_loss: 1486376876.4932\n",
      "Epoch 961/1000\n",
      "1168/1168 [==============================] - 0s 162us/step - loss: 308152162.7740 - val_loss: 1480697909.9178\n",
      "Epoch 962/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 320723240.1712 - val_loss: 1504331178.6301\n",
      "Epoch 963/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 330693878.3288 - val_loss: 1671521130.8493\n",
      "Epoch 964/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 315392817.8630 - val_loss: 1656426805.9589\n",
      "Epoch 965/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 294338794.4384 - val_loss: 1632713895.7808\n",
      "Epoch 966/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 294760717.5137 - val_loss: 1599611327.7260\n",
      "Epoch 967/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 322810093.5411 - val_loss: 1470036106.5205\n",
      "Epoch 968/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 289508076.8767 - val_loss: 1515042357.2603\n",
      "Epoch 969/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 309030656.3630 - val_loss: 1448720362.4110\n",
      "Epoch 970/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 299028152.0479 - val_loss: 1485005466.6301\n",
      "Epoch 971/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 302600154.5137 - val_loss: 1476218532.7123\n",
      "Epoch 972/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 286417328.5068 - val_loss: 1640050952.3288\n",
      "Epoch 973/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 341909154.1575 - val_loss: 1570687222.0274\n",
      "Epoch 974/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 124us/step - loss: 314419770.8493 - val_loss: 1551601547.3973\n",
      "Epoch 975/1000\n",
      "1168/1168 [==============================] - 0s 143us/step - loss: 330414193.7055 - val_loss: 1467561532.3836\n",
      "Epoch 976/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 288765733.8082 - val_loss: 1477492126.7945\n",
      "Epoch 977/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 279669997.1027 - val_loss: 1483035932.0548\n",
      "Epoch 978/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 285113184.1370 - val_loss: 1611624418.0274\n",
      "Epoch 979/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 297134878.0411 - val_loss: 1599051691.1781\n",
      "Epoch 980/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 294356238.1164 - val_loss: 1536130563.1233\n",
      "Epoch 981/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 304158100.0890 - val_loss: 1697321841.6438\n",
      "Epoch 982/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 290522239.9315 - val_loss: 1449756490.4658\n",
      "Epoch 983/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 304846193.0479 - val_loss: 1628384643.0137\n",
      "Epoch 984/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 315515022.6438 - val_loss: 1619888468.9315\n",
      "Epoch 985/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 293037806.2740 - val_loss: 1538582207.5616\n",
      "Epoch 986/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 308841980.7123 - val_loss: 1583831320.6027\n",
      "Epoch 987/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 311884877.083 - 0s 120us/step - loss: 317502325.4178 - val_loss: 1495581932.0548\n",
      "Epoch 988/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 310807161.6986 - val_loss: 1570459369.6986\n",
      "Epoch 989/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 315865494.9452 - val_loss: 1388194162.6301\n",
      "Epoch 990/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 289518566.5479 - val_loss: 1458864359.7808\n",
      "Epoch 991/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 303568858.4932 - val_loss: 1808581024.0000\n",
      "Epoch 992/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 312669326.5548 - val_loss: 1608785340.8767\n",
      "Epoch 993/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 307439622.7603 - val_loss: 1622753311.1233\n",
      "Epoch 994/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 304275479.7671 - val_loss: 1560766709.4795\n",
      "Epoch 995/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 315429839.2740 - val_loss: 1536169621.9178\n",
      "Epoch 996/1000\n",
      "1168/1168 [==============================] - 0s 213us/step - loss: 297275686.8151 - val_loss: 1619031855.7808\n",
      "Epoch 997/1000\n",
      "1168/1168 [==============================] - 0s 156us/step - loss: 312201768.7808 - val_loss: 1588187395.8356\n",
      "Epoch 998/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 297081936.6301 - val_loss: 1609087528.7671\n",
      "Epoch 999/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 325697286.2740 - val_loss: 1566873945.4247\n",
      "Epoch 1000/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 309366190.1027 - val_loss: 1519567854.7945\n"
     ]
    }
   ],
   "source": [
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 50, init = 'he_uniform',activation='relu',input_dim = 176))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 25, init = 'he_uniform',activation='relu'))\n",
    "\n",
    "# Adding the third hidden layer\n",
    "classifier.add(Dense(output_dim = 50, init = 'he_uniform',activation='relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'he_uniform'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(loss='mean_squared_error', optimizer='Adamax')\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "model_history=classifier.fit(df_Train.values, y,validation_split=0.20, batch_size = 10, nb_epoch = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_pred=classifier.predict(df_Test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_pred= ann_pred.reshape(1459,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1459,), (1459, 1))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape, ann_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[122139.45],\n",
       "       [178495.11],\n",
       "       [193507.62],\n",
       "       ...,\n",
       "       [179898.19],\n",
       "       [117205.2 ],\n",
       "       [203231.97]], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ann= []\n",
    "\n",
    "\n",
    "for i in ann_pred:\n",
    "    predict_ann.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission2= pd.DataFrame({\n",
    "    \"Id\": s['Id'],\n",
    "    \"SalePrice\": predict_ann\n",
    "})\n",
    "\n",
    "submission2.to_csv('submission_ann.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
